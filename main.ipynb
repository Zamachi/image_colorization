{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset uzet sa [link](https://www.kaggle.com/datasets/shravankumar9892/image-colorization)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  from google.colab import output\n",
    "  output.enable_custom_widget_manager()\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "IN_COLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    %pip install torchvision torchaudio torch scipy numpy scikit-image lion-pytorch torchmetrics\n",
    "DEBUG=False\n",
    "if DEBUG:\n",
    "  import os\n",
    "  os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path='./dataset/'\n",
    "model_weights_path = './model_weights/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\CompVisProject\\.venv\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray\n",
    "from skimage.io import imsave\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import math, time\n",
    "from tqdm.auto import tqdm\n",
    "from lion_pytorch import Lion\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from gc import collect\n",
    "from os.path import isfile, exists\n",
    "from os import remove, makedirs \n",
    "from psutil import virtual_memory\n",
    "from torchvision import transforms\n",
    "from warnings import warn, filterwarnings\n",
    "import zipfile \n",
    "from itertools import islice\n",
    "# import kaggle.api\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "seed=42\n",
    "_ = torch.manual_seed(seed)\n",
    "_np = np.random.seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    _cuda = torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_classes=26\n",
    "# start_ab = -128\n",
    "# end_ab = 128\n",
    "# bins = np.linspace(start_ab, end_ab, num=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mappings = dict()\n",
    "\n",
    "# class_counter = 0\n",
    "# for i in range(num_classes):\n",
    "#   for j in range(num_classes):\n",
    "#     mappings[str(i) + str(j)] = class_counter\n",
    "#     class_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE verovatnoce iz \"Colorful Image Colorization\" rada\n",
    "probs = np.load('./prior_probs.npy', mmap_mode='r')\n",
    "mappings = np.load('./pts_in_hull.npy', mmap_mode='r')\n",
    "num_classes = len(probs)\n",
    "ab_range = np.arange(-110, 120, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where( np.all( mappings == [-90,50], axis=1) )[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.histogram2d(bins,bins,bins=26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gde_sam_stao=0\n",
    "# brojac=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probsNotFormed = False \n",
    "# if probsNotFormed:\n",
    "#   from datasets import load_dataset\n",
    "#   dataset = load_dataset('imagenet-1k', 'en', split='train', streaming=True)\n",
    "#   if not isfile(f'{dataset_path}distributions.npy'):\n",
    "#       distros = np.memmap(f'{dataset_path}distributions.npy',mode=\"w+\",dtype=np.ulonglong,shape=(num_classes ** 2))\n",
    "#   else:\n",
    "#       distros = np.memmap(f'{dataset_path}distributions.npy',dtype=np.ulonglong, mode=\"r+\")\n",
    "#   for sth in dataset:\n",
    "#     if brojac != gde_sam_stao:\n",
    "#       brojac+=1\n",
    "#       continue\n",
    "#     if np.array(sth['image']).ndim == 3:\n",
    "#       lab_image = rgb2lab( (np.array(sth['image'].resize((224,224))).reshape(224,224,3))/255, channel_axis=2)\n",
    "#       quantized_a = np.digitize(lab_image[:,:,1], bins=bins)-1\n",
    "#       quantized_b = np.digitize(lab_image[:,:,2], bins=bins)-1\n",
    "#       for i in range(224):\n",
    "#         for j in range(224):\n",
    "#           distros[mappings[str(quantized_a[i,j]) + str(quantized_b[i,j])]] += 1\n",
    "#           brojac+=1\n",
    "#           gde_sam_stao = brojac\n",
    "\n",
    "#   distros.flush()\n",
    "#   del distros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distros = np.memmap(f'{dataset_path}distributions.npy',dtype=np.ulonglong, mode=\"r\")\n",
    "# probs = distros / sum(distros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices_larger_than_0 = np.where( probs > 0)\n",
    "# probs = indices_larger_than_0[0]\n",
    "# num_classes = len(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isfile('./archive.zip'):\n",
    "    kaggle.api.authenticate()\n",
    "    kaggle.api.dataset_download_files('shravankumar9892/image-colorization',path='.',unzip=False)\n",
    "\n",
    "if not exists(dataset_path):\n",
    "    makedirs(dataset_path, exist_ok=True)\n",
    "if not exists(model_weights_path):\n",
    "    makedirs(model_weights_path, exist_ok=True)\n",
    "\n",
    "if not exists(dataset_path+'ab') and not exists(dataset_path+'l'):\n",
    "    with zipfile.ZipFile('./archive.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(dataset_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funkcija za konvertovanje slike iz sRGB u L\\*a\\*b\\*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rgb_to_lab(image: np.ndarray) -> np.ndarray: \n",
    "    \"\"\"\n",
    "    Ova funkcija konvertuje image iz sRGB prostora u L*a*b* prostor.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    image : np.NDArray, shape: Tuple[int,int,int[,int]]\n",
    "        Slika(ili slike) treba da bude 3D/4D numpy array, gde je prva dimenzija redni broj slike(ako ih ima vise), druga dimenzija broj kanala, a treca i cetvrta sirina x visina. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        Returns a `np.ndarray` of shape (num_of_images, 3, height, width) if `image` is 4D, else if `image` is 3D it returns (3, height, width).\n",
    "    \"\"\"\n",
    "    assert 3 in image.shape, f\"Nije pronadjena nijedna dimenzija koja je =3\"\n",
    "    assert 3 <= image.ndim <= 4, f\"Ocekivani broj dimenzija ulaznog parametra je izmedju 3 i 4(inclusive), a dobijeno je {image.ndim}\"\n",
    "    assert np.all( (image >= 0) & (image <= 255 )), f\"Ocekivano da slike budu RGB formata s opsegom [0,255], to nije dobijeno ovde...\"\n",
    "    if np.all( (image == 0) | (image == 255)):\n",
    "        warn(f\"Postoje slike koje su iskljucivo bele ili crne\")\n",
    "    if image.ndim == 3:\n",
    "        assert image.shape[0] == 3, f\"Pogresna dimenzija na poziciji shape[0], ocekivano 3, dobijeno {image.shape[0]}\"\n",
    "        lab_image = rgb2lab(1.0/255*image, channel_axis=0)\n",
    "        lab_image[1:] = lab_image[1:] / 128\n",
    "        # X = lab_image[0]\n",
    "        # Y = lab_image[1:]\n",
    "        assert np.all( ( lab_image[0] >= 0) & ( lab_image[0] <= 100) ), f\"U L* kanalu pronadjeno nedozvoljenih vrednosti\"\n",
    "        assert np.all( ( lab_image[1:] >= -1) & ( lab_image[1:] <= 1) ), f\"U a*b* kanalima pronadjeno nedozvoljenih vrednosti\"\n",
    "        if np.all( (lab_image[0] == 0.0) | (lab_image[0] == 100.0) | (lab_image[1:] == 1.0) | (lab_image[1:] == -1.0 ) | (lab_image[1:] == 0.0 )):\n",
    "            warn(f\"Potencijalno postoje slike koje su iskljucivo bele ili crne\")\n",
    "    elif image.ndim == 4:\n",
    "        assert image.shape[1] == 3, f\"Pogresna dimenzija na poziciji shape[1], ocekivano 3, dobijeno {image.shape[1]}\"\n",
    "        lab_image = rgb2lab(1.0/255*image, channel_axis=1)\n",
    "        lab_image[:,1:] = lab_image[:,1:] / 128\n",
    "        assert np.all( (lab_image[:,0:1] >= 0 ) & (lab_image[:,0:1] <= 100 )), f\"U L* kanalu pronadjeno nedozvoljenih vrednosti\"\n",
    "        assert np.all( (lab_image[:,1:] >= -1 ) & (lab_image[:,1:] <= 1 )), f\"U a*b* kanalima pronadjeno nedozvoljenih vrednosti\"\n",
    "        if np.all( (lab_image[:,0:1] == 0.0) | (lab_image[:,0:1] == 100.0) | (lab_image[:,1:] == 1.0) | (lab_image[:,1:] == -1.0 ) | (lab_image[:,1:] == 0.0 ) ):\n",
    "            warn(f\"Potencijalno postoje slike koje su iskljucivo bele ili crne\")\n",
    "        # X = lab_image[:,0]\n",
    "        # Y = lab_image[:,1:]\n",
    "    return lab_image\n",
    "    # Y /= 128\n",
    "    # X = X.reshape(1 if image.ndim == 3 else image.shape[0], 1, image.shape[1 if image.ndim == 3 else 2], image.shape[2 if image.ndim == 3 else 3])\n",
    "    # assert np.all( (X >= 0) & (X <= 100) , axis=(1,2) if image.ndim == 3 else (2,3)).any(), f\"U L* kanalu pronadjeno nedozvoljenih vrednosti\"\n",
    "    # Y = Y.reshape(1 if image.ndim == 3 else image.shape[0], 2, image.shape[1 if image.ndim == 3 else 2], image.shape[2 if image.ndim == 3 else 3])\n",
    "    # assert np.all( (Y >= -1) & (Y <= 1) ), f\"U L* kanalu pronadjeno nedozvoljenih vrednosti\"\n",
    "    # return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: testiramo ispravnost\n",
    "# img = convert_rgb_to_lab(image.reshape(1,3,400,400))\n",
    "# X,Y = img[:,0:1,:,:], img[:,1:,:,:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funkcija za reverse convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lab_to_rgb(image: np.ndarray, denormalize=False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Ova funkcija konvertuje iz L*a*b* prostora u sRGB prostor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image: np.NDArray, shape: Tuple[int,int,int[,int]]\n",
    "        Slika(ili slike) treba da bude 3D/4D numpy array, gde je prva dimenzija redni broj slike(ako ih ima vise), druga dimenzija oznacava kanal(L*, a* ili b*), a treca i cetvrta sirina x visina. L* kanal mora da sadrzi vrednosti od 0 do 100, dok a* i b* moraju imati vrednosti izmedju -128 i 127.\n",
    "    denormalize : boolean=False\n",
    "        Da li denormalizovati podatke. Ako je denormalize=`True`, onda se koristi sRGB opseg [0,255], u protivnom se koristi [0,1]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Vraca nam sliku(ili slike) u `numpy.ndarray` formatu.   \n",
    "    \"\"\"\n",
    "    assert 3 in image.shape, f\"Nije pronadjena nijedna dimenzija koja je =3\"\n",
    "    assert 3 <= image.ndim <= 4, f\"Ocekivani broj dimenzija ulaznog parametra je izmedju 3 i 4(inclusive), a dobijeno je {image.ndim}\"\n",
    "    if image.ndim == 3:\n",
    "        assert image.shape[0] == 3, f\"Pogresna dimenzija na poziciji shape[0], ocekivano 3, dobijeno {image.shape[0]}\"\n",
    "        assert np.all( (image[0] >= 0.0) & ( image[0] <= 100.0) & (image[1:] >= -128) & ( image[1:] <= 128) )\n",
    "        rgb_image = lab2rgb(image, channel_axis=0)\n",
    "    elif image.ndim == 4:\n",
    "        assert image.shape[1] == 3, f\"Pogresna dimenzija na poziciji shape[1], ocekivano 3, dobijeno {image.shape[1]}\"\n",
    "        print()\n",
    "        assert np.all( (image[:,0:1] >= 0.0) & ( image[:,0:1] <= 100.0) & (image[:,1:] >= -128) & ( image[:,1:] <= 128) )\n",
    "        rgb_image = lab2rgb(image, channel_axis=1)\n",
    "    # rgb_image = rgb_image.reshape(1 if image.ndim == 3 else image.shape[0], 3, image.shape[1 if image.ndim == 3 else 2], image.shape[2 if image.ndim == 3 else 3])\n",
    "    assert np.all( (rgb_image >= 0) & (rgb_image <=1.0)), f\"Ocekivani opseg RGB vrednosti 0-1 je prekrsen\"\n",
    "    if np.all( (rgb_image == 0.0) | (rgb_image == 1.0)):\n",
    "        warn(f\"Postoje slike koje su iskljucivo crne ili bele\")\n",
    "    return rgb_image if not denormalize else (rgb_image * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: testiramo ispravnost\n",
    "# imgs_back_2rgb = convert_lab_to_rgb( np.concatenate( (X,Y), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(mmap_mode = None, percentage:int = 0.7, shape=(25000,3,224,224), returnJoinedInstead=False):\n",
    "    \"\"\"\n",
    "    Treba da ucita podatke sa diska kao memory map. Memory-mapped podaci se ne ucitavaju svi u memoriju, vec se ucitavaju sa diska direktno po potrebi. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mmap_mode : str | None\n",
    "        U kom rezimu treba da ucitamo finalni dataset. Za vise videti [link](https://numpy.org/doc/stable/reference/generated/numpy.memmap.html#numpy.memmap). Ako je `None`, ucitace ceo dataset u memoriju.\n",
    "    percentage : int = 0.7\n",
    "        Koliko procenata dostupne sistemske memorije zelimo iskoristiti za konverziju.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        Ucitani L*a*b* dataset `np.ndarray`, ako je mmap_mode=`None`, u protivnom vraca `memmap` i cita se sa diska.\n",
    "    \"\"\"\n",
    "    should_delete_dataset=False\n",
    "    assert 0.1 < percentage < 0.9, f\"Ocekivan opseg procenata [10%,90%], dobijeno {percentage}\"\n",
    "    if isfile(dataset_path + 'lab_dataset.npy'):\n",
    "        return np.memmap(dataset_path + 'lab_dataset.npy',mode=mmap_mode, shape=shape,dtype=\"float16\") if not returnJoinedInstead else np.load(dataset_path + 'joined_dataset.npy', mmap_mode='r')\n",
    "    elif isfile(dataset_path + 'joined_dataset.npy'):\n",
    "        joined_dataset = np.load(dataset_path + 'joined_dataset.npy', mmap_mode='r') # ucitavamo zdruzeni dataset\n",
    "        dataset_shape = joined_dataset.shape\n",
    "        dataset = np.memmap(dataset_path + 'lab_dataset.npy', mode=\"w+\", shape=(dataset_shape[0],3,224,224), dtype=\"float16\") # kreiramo memory-mapped fajl za finalni dataset (7 GB).\n",
    "        available_system_memory_in_GBs = virtual_memory().available/1024**3\n",
    "        how_much_memory_to_reserve_for_conversion_in_GBs = available_system_memory_in_GBs * percentage\n",
    "        converted_image_size_in_GBs = dataset_shape[0] * 8 * 3 * 224 * 224 / 1024**3  # 28 GB u sustini ako sve odjednom konvertujem.\n",
    "        print(f\"Dostupna memorija za konverziju slika {how_much_memory_to_reserve_for_conversion_in_GBs}\")\n",
    "        print(f\"Memorija potrebna za konverziju slika {converted_image_size_in_GBs}\")\n",
    "\n",
    "        try:\n",
    "            if how_much_memory_to_reserve_for_conversion_in_GBs - converted_image_size_in_GBs >= 1: # Ostavljamo 1 GB overhead-a\n",
    "                image = convert_rgb_to_lab(joined_dataset)\n",
    "                dataset[:] = image\n",
    "            else: # U protivnom koristimo batched obradu.\n",
    "                batch_size = int( (how_much_memory_to_reserve_for_conversion_in_GBs-1)*dataset_shape[0] / (32*converted_image_size_in_GBs) ) # -1 zbog memory overheada. 32 jer rgb2lab koristi float64...\n",
    "                print(f\"Batch size:\\t{batch_size}\")\n",
    "                assert batch_size > 1, f\"Nemate dovoljno memorije za ovakvu operaciju, ocekivano je da batch_size bude veci od 1, ali je {batch_size}\"\n",
    "                for i in range(0, dataset_shape[0], batch_size+1):\n",
    "                    dataset[i:i+batch_size] = convert_rgb_to_lab( joined_dataset[i:i+batch_size] )\n",
    "                dataset[i:] = convert_rgb_to_lab( joined_dataset[i:] )\n",
    "        except MemoryError as e:\n",
    "            print(f\"Doslo je do greske:\\n{e}\")\n",
    "            should_delete_dataset = True\n",
    "        except Exception as e:\n",
    "            should_delete_dataset = True\n",
    "            print(e)\n",
    "        finally:\n",
    "            dataset.flush()\n",
    "            print(\"Flushed!\")\n",
    "            del dataset, joined_dataset\n",
    "            collect()\n",
    "            if should_delete_dataset:\n",
    "                remove(dataset_path + 'lab_dataset.npy')\n",
    "                return\n",
    "\n",
    "        print(\"Kreirani finalni dataset\")\n",
    "        return load_data(mmap_mode, percentage, dataset_shape)\n",
    "    else:\n",
    "        X = np.load(dataset_path + 'l/gray_scale.npy',mmap_mode='r').reshape(25000,1,224,224)\n",
    "        y_file_to_load = [ f'ab/ab/ab{i}.npy' for i in range(1,4)]\n",
    "        Y = np.concatenate( [ np.load(dataset_path + file) for file in y_file_to_load ], axis=0 ).reshape(25000,2,224,224)\n",
    "        np.save(dataset_path + 'joined_dataset.npy', np.concatenate( (X,Y), axis=1))\n",
    "        del X,Y,y_file_to_load\n",
    "        collect()\n",
    "        print(\"Kreirani zdruzeni dataset\")\n",
    "        return load_data(mmap_mode, percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ucitavanje i pripremu dataseta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcija za ucitavanje i kreiranje memory-mapped dataseta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definisanje transformacija slika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: transformacija slike nije podrzana...\n",
    "# transform = transforms.Compose([\n",
    "#     # transforms.ToTensor(),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomAffine(degrees=20, shear=0.2, scale=(0.8, 1.2)),\n",
    "#     # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasa naseg custom dataseta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definisanje custom dataseta\n",
    "class IterableImageDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, dataset_indices: list, num_of_classes:int, mappings:dict, return_joined:bool=False, transform=None, bins=None):\n",
    "        \"\"\"\n",
    "        Inicijalizuje dataset\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_indices : list\n",
    "            Koje redove iz dataseta koristimo\n",
    "        transform(optional) : torchvision.Compose\n",
    "            Transformacije koje koje primenjujemo nad nasim slikama.\n",
    "        \"\"\"\n",
    "        # assert data.ndim == 4, f\"Ocekivan broj dimenzija dataseta 4, dobijeno {data.ndim}\"\n",
    "        # assert data.shape == (data.shape[0],3,224,224), f\"Ocekivan shape dataseta (25000,3,224,224), dobijeno {data.shape}\"\n",
    "        # assert np.all( (data >= 0) & (data <= 255)), f\"Slike treba da su u RGB formatu!\"\n",
    "        self.data = load_data(\"r\", returnJoinedInstead=return_joined) \n",
    "        self.is_joined = return_joined\n",
    "        self.indices = dataset_indices\n",
    "        self.transform = transform\n",
    "        self.num_of_classes = num_of_classes\n",
    "        self.mappings = mappings\n",
    "        self.bins = bins\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def process_data(self, dataset):\n",
    "        for idx in self.indices:\n",
    "            img = dataset[idx]\n",
    "            if self.transform:\n",
    "                # if self.is_joined:\n",
    "                #     img_transformed = convert_rgb_to_lab(img_transformed)\n",
    "                img_transformed = self.transform(torch.as_tensor( convert_lab_to_rgb(img) if not self.is_joined else img,dtype=torch.float32))\n",
    "\n",
    "                yield img_transformed[0].reshape(-1,224,224), map_ab_to_quantized(img_transformed[1:] * 110 , self.mappings, bins=self.bins)\n",
    "            else:\n",
    "                # if self.is_joined:\n",
    "                img = torch.as_tensor( convert_lab_to_rgb(img) if not self.is_joined else img,dtype=torch.float32)\n",
    "                yield img[0].reshape(-1,224,224), map_ab_to_quantized(img[1:] * 110, self.mappings, bins=self.bins)\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info() # NOTE ako je None, onda je single-process\n",
    "        if worker_info is None:\n",
    "            return iter(self.process_data(self.data))\n",
    "            # return islice(map(self.process_data, iter(self.data)),0)\n",
    "        total_number_of_workers = worker_info.num_workers\n",
    "        worker_id = worker_info.id\n",
    "        dataset_length = self.__len__()\n",
    "\n",
    "        mapped_iterator = map(self.process_data, iter(self.data))\n",
    "        # return islice(iter(self.process_data(self.data[self.indices])), worker_id, None, total_number_of_workers)\n",
    "        return islice(mapped_iterator, worker_id, None, total_number_of_workers)\n",
    " \n",
    "    #   lab_image = rgb2lab( (np.array(sth['image'].resize((224,224))).reshape(224,224,3))/255, channel_axis=2)\n",
    "    #   quantized_a = np.digitize(lab_image[:,:,1], bins=bins)\n",
    "    #   quantized_b = np.digitize(lab_image[:,:,2], bins=bins)\n",
    "def map_ab_to_quantized(ab_image, mappings: np.ndarray, bins):\n",
    "    output = torch.zeros((224, 224), dtype=torch.long)\n",
    "    a_channel = np.digitize(ab_image[0:1], bins=bins).reshape((224,224))\n",
    "    b_channel = np.digitize(ab_image[1:2], bins=bins).reshape((224,224))\n",
    "    for i in range(224):\n",
    "        for j in range(224):\n",
    "            # output[i,j] = mappings[str(a_channel[i,j]) + str(b_channel[i,j])]\n",
    "            idx = np.where( np.all( mappings == [bins[a_channel[i,j]], bins[b_channel[i,j]]], axis=1))[0]\n",
    "            output[i,j] = idx.item() if len(idx) == 1 else find_closest_color(a_channel[i,j], b_channel[i,j], bins, mappings)\n",
    "    return output\n",
    "\n",
    "def find_closest_color(a_idx:int, b_idx:int, bins, mappings) -> int:\n",
    "    min_idx = 0 \n",
    "    min_diff = None\n",
    "    for idx, mapping in enumerate(mappings):\n",
    "        distance = abs( (abs(mapping[0]) - abs(bins[a_idx]) ) ** 2 + ( abs(mapping[1]) - abs(bins[b_idx]) ) ** 2)\n",
    "        if min_diff is None or distance < min_diff:\n",
    "            min_diff = distance\n",
    "            min_idx = idx\n",
    "    return min_idx "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ucitavanje podataka, podela na trening i test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_memory_map = load_data('r')\n",
    "# dataset_memory_map = load_data('r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = torch.zeros((224, 224), dtype=torch.long)\n",
    "# shouldBreak=False\n",
    "# for img_no, img in enumerate(dataset_memory_map):\n",
    "#     ab_image = torch.as_tensor(img[1:].copy(), dtype=torch.float32) * 110\n",
    "#     a_channel = np.digitize(ab_image[0:1], bins=ab_range).reshape((224,224))\n",
    "#     b_channel = np.digitize(ab_image[1:2], bins=ab_range).reshape((224,224))\n",
    "#     for i in range(224):\n",
    "#         for j in range(224):\n",
    "#             # output[i,j] = mappings[str(a_channel[i,j]) + str(b_channel[i,j])]\n",
    "#             # idx = np.where( np.all( mappings == [bins[a_channel[i,j]], bins[b_channel[i,j]]], axis=1))[0]\n",
    "#             idx = np.where( np.all( mappings == [ab_range[a_channel[i,j]] , ab_range[b_channel[i,j]]], axis=1))[0]\n",
    "#             if len(idx) != 1:\n",
    "#                 idx = find_closest_color(a_channel)\n",
    "#                 shouldBreak = True\n",
    "#                 print(f\"Hit na slici broj {img_no}, piksel [{i},{j}], pokusana boja out-of-gamut [{ab_range[a_channel[i,j]]}, {ab_range[b_channel[i,j]]}], a zamenjena sa: {mappings[idx]}\")\n",
    "#             else:\n",
    "#                 continue\n",
    "#             # output[i,j] = idx.item() if len(idx) == 1 else find_closest_color(a_channel[i,j], b_channel[i,j], bins, mappings)\n",
    "\n",
    "#             if shouldBreak:\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: aktiviraj globalnu booleanku da testiras ovo v\n",
    "check_if_memory_map_values_valid = False \n",
    "if check_if_memory_map_values_valid:\n",
    "    use_joined=False\n",
    "    dataset_memory_map_test = load_data('r',returnJoinedInstead=use_joined)\n",
    "    for i in range(len(dataset_memory_map_test)):\n",
    "        if not use_joined:\n",
    "            assert np.all( ( dataset_memory_map_test[i][0] >= 0.0 ) & ( dataset_memory_map_test[i][0]  <= 100.0 )), f\"Ocekivan opseg u L* kanalu [0,100], ali red {i} u dataset_memory_map_test krsi ovo pravilo.\"\n",
    "            assert np.all( ( dataset_memory_map_test[i][1:] >= -1.0 ) & ( dataset_memory_map_test[i][1:]  <= 1.0 )), f\"Ocekivan opseg u a* i b* kanalu [-1,1], ali red {i} u dataset_memory_map_test krsi ovo pravilo.\"\n",
    "        else:\n",
    "            assert np.all( ( dataset_memory_map_test[i] >= 0 ) & ( dataset_memory_map_test[i] <= 255 )), f\"Ocekivan opseg u a* i b* kanalu [-1,1], ali red {i} u dataset_memory_map_test krsi ovo pravilo.\"\n",
    "        if not use_joined and ( np.all( (dataset_memory_map[i][0] == 0.0) | (dataset_memory_map[i][0] == 100.0)) or np.all( (dataset_memory_map[i][1:] == 0.0) | (dataset_memory_map[i][1:] == 1.0)| (dataset_memory_map[i][1:] == -1.0))):\n",
    "           warn(f\"Slika redni broj {i} sadrzi cudne vrednosti na L* ili a*/b* kanalima. L* treba da je [0,100](float), a a*b* [-1,1](float)\") \n",
    "        elif use_joined and np.all( (dataset_memory_map[i] == 0) | (dataset_memory_map[i] == 255) ):\n",
    "           warn(f\"Slika redni broj {i} sadrzi cudne vrednosti na RGB kanalima, opseg treba da je [0,255](uint8).\") \n",
    "\n",
    "    del dataset_memory_map_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "validation_size = 0.15\n",
    "\n",
    "test_length = int(test_size * dataset_memory_map.shape[0])\n",
    "remaining_length = dataset_memory_map.shape[0] - test_length\n",
    "validation_length = int(remaining_length * validation_size)\n",
    "training_length = remaining_length - validation_length\n",
    "\n",
    "training_length, validation_length, test_length\n",
    "\n",
    "indices = [i for i in range(dataset_memory_map.shape[0])]\n",
    "\n",
    "training_indices = np.random.choice(range(0, dataset_memory_map.shape[0]), size=training_length, replace=False)\n",
    "validation_indices = np.random.choice(list(set(range(0, dataset_memory_map.shape[0])) - set(training_indices)), size=validation_length, replace=False)\n",
    "test_indices = np.random.choice(list(set(range(0, dataset_memory_map.shape[0])) - set(training_indices) - set(validation_indices)), size=test_length, replace=False)\n",
    "\n",
    "# training_ds = IterableImageDataset(dataset_memory_map[training_indices], transform=transform)\n",
    "# validation_ds = IterableImageDataset(dataset_memory_map[validation_indices], transform=transform)\n",
    "training_ds = IterableImageDataset(training_indices, mappings=mappings, num_of_classes=num_classes, bins=ab_range)\n",
    "validation_ds = IterableImageDataset(validation_indices, mappings=mappings, num_of_classes=num_classes, bins=ab_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Annealed mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxAnnealedMean(torch.nn.Module):\n",
    "    def __init__(self, temperature:float=0.38):\n",
    "        super(SoftmaxAnnealedMean, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, ground_truth, dim:int=1, isLogits:bool=True):\n",
    "        \"\"\"\n",
    "        Sracunava annealed mean raspodele verovatnoca. Ovo koristimo da predvidjenu distribuciju mapiramo u tacku u ab prostoru.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ground_truth : torch.Tensor\n",
    "            Istinitosne vrednosti za ab kanale oblika (batch_size, number_of_classes, width, height)\n",
    "        dim : int\n",
    "            Dimenzija po kojoj se vrsi sumiranje, ako je broj klasa na drugoj poziciji, koristiti defaultnu vrednost `dim=1`.\n",
    "        isLogits : bool = True\n",
    "            Da li su vrednosti u prosledjenom tenzoru logiti ili ne - ako jesu, konvertovace ih u verovatnoce automatski.\n",
    "        \"\"\"\n",
    "        if isLogits:\n",
    "            ground_truth = torch.nn.Softmax(ground_truth, dim=dim)\n",
    "        return torch.exp( torch.log(ground_truth) / self.temperature ) / torch.sum( torch.exp( torch.log(ground_truth) / self.temperature ) , dim=dim)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizerModel(nn.Module):\n",
    "    def __init__(self, number_of_classes:int):\n",
    "        super(ColorizerModel, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1, stride=2)\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=2)\n",
    "        self.conv7 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv8 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.conv9 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.upsample1 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.conv10 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.upsample2 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.conv11 = nn.Conv2d(64, 32, kernel_size=3, padding=1)\n",
    "        self.upsample3 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        # self.last = nn.Conv2d(n_prev, number_of_classes, kernel_size=1, stride=1, padding=0)\n",
    "        # self.model_output = nn.Conv2d(number_of_classes,2, kernel_size=1,padding=0,dilation=1,stride=1,bias=False)\n",
    "        self.model_output = nn.Conv2d(32,number_of_classes, kernel_size=1,padding=0,dilation=1,stride=1, bias=False)\n",
    "        # self.softmax = SoftmaxAnnealedMean()\n",
    "        # self.ab_output = nn.Conv2d(number_of_classes, 2, kernel_size=1, padding=0, dilation=1, stride=1, bias=False)\n",
    "        # self.upsample4 = nn.Upsample(scale_factor=4, mode='bilinear')\n",
    "\n",
    "    def forward(self, x, isInference:bool=False):\n",
    "        x = nn.ReLU()(self.conv1(x))\n",
    "        x = nn.ReLU()(self.conv2(x))\n",
    "        x = nn.ReLU()(self.conv3(x))\n",
    "        x = nn.ReLU()(self.conv4(x))\n",
    "        x = nn.ReLU()(self.conv5(x))\n",
    "        x = nn.ReLU()(self.conv6(x))\n",
    "        x = nn.ReLU()(self.conv7(x))\n",
    "        x = nn.ReLU()(self.conv8(x))\n",
    "        x = nn.ReLU()(self.conv9(x))\n",
    "        x = self.upsample1(x)\n",
    "        x = nn.ReLU()(self.conv10(x))\n",
    "        x = self.upsample2(x)\n",
    "        x = nn.ReLU()(self.conv11(x))\n",
    "        # x = self.tanh(self.conv12(x))\n",
    "        x = self.upsample3(x)\n",
    "        x = self.model_output(x)\n",
    "        return x\n",
    "        # if not isInference:\n",
    "        #     return x\n",
    "        # else:\n",
    "        #     return self.upsample4(self.ab_output(self.softmax(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ColorizerModel(number_of_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input=torch.rand((16,1,224,224),dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_output = nn.ReLU()(model.conv1(test_input))\n",
    "conv2_output = nn.ReLU()(model.conv2(conv1_output))\n",
    "conv3_output = nn.ReLU()(model.conv3(conv2_output))\n",
    "conv4_output = nn.ReLU()(model.conv4(conv3_output))\n",
    "conv5_output = nn.ReLU()(model.conv5(conv4_output))\n",
    "conv6_output = nn.ReLU()(model.conv6(conv5_output))\n",
    "conv7_output = nn.ReLU()(model.conv7(conv6_output))\n",
    "conv8_output = nn.ReLU()(model.conv8(conv7_output))\n",
    "conv9_output = nn.ReLU()(model.conv9(conv8_output))\n",
    "upsample1_output = model.upsample1(conv9_output)\n",
    "conv10_output = nn.ReLU()(model.conv10(upsample1_output))\n",
    "upsample2_output = model.upsample2(conv10_output)\n",
    "conv11_output = nn.ReLU()(model.conv11(upsample2_output))\n",
    "upsample3_output = model.upsample3(conv11_output)\n",
    "model_output = model.model_output(upsample3_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1_output has output of shape:\t torch.Size([16, 64, 224, 224])\n",
      "conv2_output has output of shape:\t torch.Size([16, 64, 112, 112])\n",
      "conv3_output has output of shape:\t torch.Size([16, 128, 112, 112])\n",
      "conv4_output has output of shape:\t torch.Size([16, 128, 56, 56])\n",
      "conv5_output has output of shape:\t torch.Size([16, 256, 56, 56])\n",
      "conv6_output has output of shape:\t torch.Size([16, 256, 28, 28])\n",
      "conv7_output has output of shape:\t torch.Size([16, 512, 28, 28])\n",
      "conv8_output has output of shape:\t torch.Size([16, 256, 28, 28])\n",
      "conv9_output has output of shape:\t torch.Size([16, 128, 28, 28])\n",
      "upsample1_output has output of shape:\t torch.Size([16, 128, 56, 56])\n",
      "conv10_output has output of shape:\t torch.Size([16, 64, 56, 56])\n",
      "upsample2_output has output of shape:\t torch.Size([16, 64, 112, 112])\n",
      "conv11_output has output of shape:\t torch.Size([16, 32, 112, 112])\n",
      "upsample3_output has output of shape:\t torch.Size([16, 32, 224, 224])\n",
      "model_output has output of shape:\t torch.Size([16, 313, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(\"conv1_output has output of shape:\\t\" , conv1_output.shape)\n",
    "print(\"conv2_output has output of shape:\\t\" , conv2_output.shape)\n",
    "print(\"conv3_output has output of shape:\\t\" , conv3_output.shape)\n",
    "print(\"conv4_output has output of shape:\\t\" , conv4_output.shape)\n",
    "print(\"conv5_output has output of shape:\\t\" , conv5_output.shape)\n",
    "print(\"conv6_output has output of shape:\\t\" , conv6_output.shape)\n",
    "print(\"conv7_output has output of shape:\\t\" , conv7_output.shape)\n",
    "print(\"conv8_output has output of shape:\\t\" , conv8_output.shape)\n",
    "print(\"conv9_output has output of shape:\\t\" , conv9_output.shape)\n",
    "print(\"upsample1_output has output of shape:\\t\" , upsample1_output.shape)\n",
    "print(\"conv10_output has output of shape:\\t\" , conv10_output.shape)\n",
    "print(\"upsample2_output has output of shape:\\t\" , upsample2_output.shape)\n",
    "print(\"conv11_output has output of shape:\\t\" , conv11_output.shape)\n",
    "print(\"upsample3_output has output of shape:\\t\" , upsample3_output.shape)\n",
    "print(\"model_output has output of shape:\\t\" , model_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "class ReweightedCrossEntropy(torch.nn.Module):\n",
    "    def __init__(self, Q_value:int, class_weights_probs:np.ndarray, lambda_value=0.5, sigma_value=5, device='cpu'):\n",
    "        super(ReweightedCrossEntropy, self).__init__()\n",
    "        self.lambda_value = lambda_value\n",
    "        self.sigma_value = sigma_value\n",
    "        self.Q_value = Q_value\n",
    "        self.class_weights_probs = class_weights_probs\n",
    "        self.device =device\n",
    "\n",
    "    def forward(self, model_predictions: torch.Tensor, ground_truth: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Metoda uzima predikcije modela, koji su nenormalizovani logiti, oblika (N, C, d1, d2 ... dK), gde je K>=1, gde je C broj klasa, a N batch size i daje reweighted cross entropy loss i istinitosne vrednosti oblika (N,d1,d2 ... dK), gde je K>=1, gde je svaka vrednost u opsegu [0,C). \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model_predictions : torch.Tensor\n",
    "            Predikcije koje daje model(nenormalizovani logiti), oblika (N, C, d1, d2 ... dK), gde je K>=1\n",
    "        ground_truth : torch.Tensor\n",
    "            Konkretan tenzor klasa za dati piksel, sadrzi **indekse** klasa, tj. svaka vrednost je [0,C), oblika (N, d1, d2 ... dK), gde je K>=1. \n",
    "        \"\"\"\n",
    "        smoothed_distribution = gaussian_filter1d(self.class_weights_probs, sigma=self.sigma_value)\n",
    "        w_value = ( (1-self.lambda_value) * smoothed_distribution + (self.lambda_value/self.Q_value) ) ** -1\n",
    "\n",
    "        return cross_entropy(model_predictions,ground_truth,torch.as_tensor(w_value, device=self.device, dtype=torch.float32))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping(object):\n",
    "    def __init__(self, mode='min', min_delta=0, patience=10, percentage=False, also_use_timer=False, seconds_to_terminate:int=60*60):\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = 0\n",
    "        self.is_better = None\n",
    "        self.also_use_timer=also_use_timer\n",
    "        self._init_is_better(mode, min_delta, percentage)\n",
    "\n",
    "        if patience == 0:\n",
    "            self.is_better = lambda a, b: True\n",
    "            self.step = lambda a: False\n",
    "\n",
    "        if also_use_timer:\n",
    "            self.start_time=time.perf_counter()\n",
    "            self.end_time = 0\n",
    "            self.time_compare = lambda start,end: end-start >= seconds_to_terminate # NOTE Terminate after an hour\n",
    "        else:\n",
    "            self.start_time=None\n",
    "            self.end_time=None\n",
    "            self.time_compare = lambda start,end: False\n",
    "\n",
    "    def step(self, metrics):\n",
    "        if self.best is None:\n",
    "            self.best = metrics\n",
    "            return False\n",
    "\n",
    "        if torch.is_tensor(metrics):\n",
    "            if torch.isnan(metrics):\n",
    "                return True\n",
    "        elif type(metrics) == float and math.isnan(metrics):\n",
    "              return True\n",
    "\n",
    "        if self.is_better(metrics, self.best):\n",
    "            self.num_bad_epochs = 0\n",
    "            self.best = metrics\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            print('terminating because of early stopping!')\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def time_ran_out(self):\n",
    "        if self.time_compare(self.start_time, self.end_time):\n",
    "            print(\"Terminating because of training time limit.\")\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _init_is_better(self, mode, min_delta, percentage):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError('mode ' + mode + ' is unknown!')\n",
    "        if not percentage:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - min_delta\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + min_delta\n",
    "        else:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - (\n",
    "                            best * min_delta / 100)\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + (\n",
    "                            best * min_delta / 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for training and evaluating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, validation: DataLoader, device, metric, is_called_from_training=False):\n",
    "    model.eval()\n",
    "    fid = [] \n",
    "    for step, batch in enumerate(validation):\n",
    "        with torch.no_grad():\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            metric.update(torch.as_tensor(convert_lab_to_rgb(torch.cat( (inputs,targets*128), dim=1).cpu().numpy(),denormalize=True),dtype=torch.uint8), real=True)\n",
    "            metric.update(torch.as_tensor(convert_lab_to_rgb(torch.cat( (inputs,outputs*128), dim=1).cpu().numpy(),denormalize=True),dtype=torch.uint8), real=False)\n",
    "            fid.append(metric.compute().item())\n",
    "    model.train()\n",
    "    del batch\n",
    "    collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return fid\n",
    "\n",
    "def fit(model: nn.Module, optimizer: optim.Optimizer, training:DataLoader, validation: DataLoader, scheduler: lr_scheduler.LRScheduler, metric_for_early_stopping,  epochs:int=50, loss_fn=nn.MSELoss(), gradient_accumulation_steps:int=8,enable_early_stopping:bool=True,patience:int=7,early_stopping_mode:str='min',delta_for_early_stopping:float=0,best:float=None,also_use_timer_for_early_stopping:bool=False, seconds_for_early_stopping:int=60*60, device:str='cpu', epoch_start:int=0, loss=None, shouldEvaluate:bool = True):\n",
    "    \"\"\"\n",
    "    Trenira/fituje model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        Ovo je objekat instanciranog modela kojeg treniramo\n",
    "    optimizer : optim.Optimizer\n",
    "        Optimizator parametara `model` koje koristimo\n",
    "    training : DataLoader\n",
    "        DataLoader za trening\n",
    "    validation : DataLoader\n",
    "        DataLoader za validaciju\n",
    "    epochs : int, optional\n",
    "        Broj epoha prilikom treninga\n",
    "    loss_fn : optional\n",
    "        Funkcija za generisanja loss-a tokom treninga `model`-a.\n",
    "    scheduler : LRSCheduler \n",
    "        Scheduler za `learning_rate` \n",
    "    gradient_accumulation_steps : int, optional\n",
    "        Koliko step-ova akumuliramo gradijente pre nego sto uradimo apdejt vejtova. Ako ne zelimo akumuliranje gradijenata, setovati ovaj parametar na 1.\n",
    "    early_stopping_mode : str, optional\n",
    "        Rezim rada early stopping mehanizma(moze biti `min` ili `max`)\n",
    "    patience : int, optional\n",
    "        Koliko koraka u EarlyStoppingu tolerisemo pre nego sto prekinemo trening\n",
    "    delta_for_early_stopping : float, optional\n",
    "        Tolerancija odstupanja performansi za early stopping\n",
    "    metric_for_early_stopping : str, optional\n",
    "        Koju metriku cemo koristiti za early stopping. \n",
    "    best : float, optional\n",
    "        Najbolji rezultat koji je model postigao. Podrazumevano nema, ako instanciramo model od 0.\n",
    "    also_use_timer_for_early_stopping : bool, optional\n",
    "        Da li se koristi i tajmer za early stopping(ako npr. zelimo da trening traje odredjeno vreme)\n",
    "    device : {'cpu', 'cuda'}\n",
    "        Na kojem uredjaju zelimo da se vrsi trening.\n",
    "    epoch_start : int=0\n",
    "        Od koje epohe poceti trening, koristi se samo ako nastavljamo od checkpointa.\n",
    "    loss \n",
    "        Koji je loss bio na poslednjoj sacuvanoj epohi?\n",
    "    \"\"\"\n",
    "    # model = model.to(device)\n",
    "\n",
    "    trainingSteps = epochs * len(training)\n",
    "    \n",
    "    if enable_early_stopping:\n",
    "        earlyStopping = EarlyStopping(patience=min(epochs, patience), mode=early_stopping_mode, min_delta=delta_for_early_stopping,also_use_timer=also_use_timer_for_early_stopping, seconds_to_terminate=seconds_for_early_stopping)\n",
    "    best = best\n",
    "    \n",
    "    progress_bar = tqdm(range(trainingSteps))\n",
    "\n",
    "    model.train()\n",
    "    completed_steps = 0\n",
    "    # Ovo navodno ubrzava\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \"\"\"\n",
    "    Reason for turning off above:\n",
    "\n",
    "    Setting torch.backends.cudnn.benchmark = True before the training loop can accelerate the computation. Because the performance of cuDNN algorithms to compute the convolution of different kernel sizes varies, the auto-tuner can run a benchmark to find the best algorithm (current algorithms are [these](https://github.com/pytorch/pytorch/blob/dab5e2a23ed387046d99f825e0d9a45bd58fccaa/aten/src/ATen/native/cudnn/Conv_v7.cpp#L268-L275), [these](https://github.com/pytorch/pytorch/blob/dab5e2a23ed387046d99f825e0d9a45bd58fccaa/aten/src/ATen/native/cudnn/Conv_v7.cpp#L341-L346), and [these](https://github.com/pytorch/pytorch/blob/dab5e2a23ed387046d99f825e0d9a45bd58fccaa/aten/src/ATen/native/cudnn/Conv_v7.cpp#L413-L418)). It's recommended to use turn on the setting when your input size doesn't change often. If the input size changes often, the auto-tuner needs to benchmark too frequently, which might hurt the performance. It can speed up by 1.27x to 1.70x for forward and backward propagation [ref](https://tigress-web.princeton.edu/~jdh4/PyTorchPerformanceTuningGuide_GTC2021.pdf).    \n",
    "    \"\"\" \n",
    "    fid_over_training = dict()\n",
    "    losses = []\n",
    "    loss_now = loss\n",
    "    loss_before = None\n",
    "    for epoch in range(epoch_start, epochs):\n",
    "        for step, batch in enumerate(training, start=1):\n",
    "            # outputs = model(**batch)\n",
    "            # loss = outputs.loss\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            # print(inputs)\n",
    "            loss = loss_fn(outputs, targets.long())\n",
    "            loss_before = loss_now\n",
    "            loss_now = loss\n",
    "            losses.append(loss.item())\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            progress_bar.update(1)\n",
    "            if step % gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad(set_to_none=True) # set_to_none ne okida memset funkciju i navodno je to brze\n",
    "                completed_steps += 1\n",
    "        evaluation = evaluate(model, validation, device, metric_for_early_stopping, is_called_from_training=True) if shouldEvaluate else best\n",
    "        if shouldEvaluate:\n",
    "            evaluation = sum(evaluation) / len(evaluation)\n",
    "            fid_over_training[epoch] = evaluation\n",
    "        print(f\"Tokom epohe {epoch+1} loss je bio {loss} sa akumuliranjem, tj. {loss*gradient_accumulation_steps} bez akumuliranja gradijenta, learning rate je {scheduler.get_last_lr()}, a evaluacija je dala metriku {evaluation}\")\n",
    "        del batch\n",
    "        collect()\n",
    "        torch.cuda.empty_cache() \n",
    "        if enable_early_stopping:\n",
    "            earlyStopping.end_time = time.perf_counter()\n",
    "        if best is None or evaluation < best or not shouldEvaluate and loss_now < loss_before:\n",
    "            best = evaluation\n",
    "            torch.save({\n",
    "                \"model\":model.state_dict(),\n",
    "                f\"optimizer_{optimizer.__class__.__name__}\" : optimizer.state_dict(),\n",
    "                f\"scheduler_{scheduler.__class__.__name__}\" : scheduler.state_dict(),\n",
    "                \"epoch\": epoch,\n",
    "                \"loss\": loss.item() * gradient_accumulation_steps,\n",
    "                \"best\" : best\n",
    "            },model_weights_path + 'checkpoint.tar')\n",
    "            # torch.save(model.state_dict(), model_weights_path+'model_weights.pth') # NOTE: Mozda puca\n",
    "            # torch.save(optimizer.state_dict(), model_weights_path+f'optimizer_{optimizer.__class__.__name__}.pt') # NOTE: Mozda puca\n",
    "            # torch.save(scheduler.state_dict(), model_weights_path+f'scheduler_{scheduler.__class__.__name__}.pt') # NOTE: Mozda puca\n",
    "\n",
    "        if enable_early_stopping and (earlyStopping.step(evaluation) or earlyStopping.time_ran_out()):\n",
    "            break\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    return {\"fid\":fid_over_training, \"losses\":losses}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_epochs = 25\n",
    "batch_size = 32\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ColorizerModel(number_of_classes=num_classes).to(device)\n",
    "optimizer = Lion(model.parameters(), lr=3.67*0.001)\n",
    "learning_rate_scheduler = lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=number_of_epochs * len(training_ds), eta_min=0.01)\n",
    "# optimizer = torch.optim.RMSprop(model.parameters())\n",
    "# learning_rate_scheduler = lr_scheduler.LinearLR(optimizer=optimizer, start_factor=0.9,end_factor=1/5,total_iters=number_of_epochs * len(training))\n",
    "\n",
    "for state in optimizer.state.values():\n",
    "    for k,v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to(device)\n",
    "\n",
    "#NOTE: menjaj ovo ako hoces da ucitas weightove\n",
    "use_pretrained_weights = False \n",
    "# training = DataLoader(training_ds, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "# validation = DataLoader(validation_ds, batch_size=int(batch_size/4), shuffle=True, pin_memory=True)\n",
    "training = DataLoader(training_ds, batch_size=batch_size, pin_memory=True)\n",
    "validation = DataLoader(validation_ds, batch_size=batch_size, pin_memory=True)\n",
    "metric = FrechetInceptionDistance(feature=64, reset_real_features=False)\n",
    "epoch_start=0\n",
    "loss_start=None\n",
    "best_result = None\n",
    "if use_pretrained_weights:\n",
    "    checkpoint = torch.load(model_weights_path+\"checkpoint.tar\", map_location=device)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint[f'optimizer_{optimizer.__class__.__name__}'])\n",
    "    for state in optimizer.state.values():\n",
    "        for key, value in state.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                state[key] = value.to(device)\n",
    "    learning_rate_scheduler.load_state_dict(checkpoint[f'scheduler_{learning_rate_scheduler.__class__.__name__}'])\n",
    "    # for state in learning_rate_scheduler.state_dict().values():\n",
    "    #     for key, value in state.items():\n",
    "    #         if isinstance(value, torch.Tensor):\n",
    "    #             state[key] = value.to(device)\n",
    "    epoch_start = checkpoint['epoch']\n",
    "    loss_start = checkpoint['loss'] \n",
    "    best_result = checkpoint['best']\n",
    "    model.eval()\n",
    "    # model.load_state_dict(torch.load(model_weights_path+ 'model_weights.pth'))\n",
    "    # optimizer.load_state_dict(torch.load(model_weights_path + f'optimizer_{optimizer.__class__.__name__}.pt', map_location=device))\n",
    "    # learning_rate_scheduler.load_state_dict(torch.load(model_weights_path + f'scheduler_{learning_rate_scheduler.__class__.__name__}.pt', map_location=device))\n",
    "\n",
    "del training_ds, validation_ds, dataset_memory_map\n",
    "collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 None None\n"
     ]
    }
   ],
   "source": [
    "print(epoch_start,loss_start,best_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filterwarnings(\"ignore\")\n",
    "# model.to(device)\n",
    "# test_real, test_pred = evaluate(model,validation,device,metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Struktura i parametri modela prikazani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ColorizerModel(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv6): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv7): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv8): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv9): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (upsample1): Upsample(scale_factor=2.0, mode='nearest')\n",
      "  (conv10): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (upsample2): Upsample(scale_factor=2.0, mode='nearest')\n",
      "  (conv11): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (upsample3): Upsample(scale_factor=2.0, mode='nearest')\n",
      "  (model_output): Conv2d(32, 313, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parametar:\tconv1.weight,\tTip:\ttorch.float32,\tVelicina:\t576\n",
      "Parametar:\tconv1.bias,\tTip:\ttorch.float32,\tVelicina:\t64\n",
      "Parametar:\tconv2.weight,\tTip:\ttorch.float32,\tVelicina:\t36864\n",
      "Parametar:\tconv2.bias,\tTip:\ttorch.float32,\tVelicina:\t64\n",
      "Parametar:\tconv3.weight,\tTip:\ttorch.float32,\tVelicina:\t73728\n",
      "Parametar:\tconv3.bias,\tTip:\ttorch.float32,\tVelicina:\t128\n",
      "Parametar:\tconv4.weight,\tTip:\ttorch.float32,\tVelicina:\t147456\n",
      "Parametar:\tconv4.bias,\tTip:\ttorch.float32,\tVelicina:\t128\n",
      "Parametar:\tconv5.weight,\tTip:\ttorch.float32,\tVelicina:\t294912\n",
      "Parametar:\tconv5.bias,\tTip:\ttorch.float32,\tVelicina:\t256\n",
      "Parametar:\tconv6.weight,\tTip:\ttorch.float32,\tVelicina:\t589824\n",
      "Parametar:\tconv6.bias,\tTip:\ttorch.float32,\tVelicina:\t256\n",
      "Parametar:\tconv7.weight,\tTip:\ttorch.float32,\tVelicina:\t1179648\n",
      "Parametar:\tconv7.bias,\tTip:\ttorch.float32,\tVelicina:\t512\n",
      "Parametar:\tconv8.weight,\tTip:\ttorch.float32,\tVelicina:\t1179648\n",
      "Parametar:\tconv8.bias,\tTip:\ttorch.float32,\tVelicina:\t256\n",
      "Parametar:\tconv9.weight,\tTip:\ttorch.float32,\tVelicina:\t294912\n",
      "Parametar:\tconv9.bias,\tTip:\ttorch.float32,\tVelicina:\t128\n",
      "Parametar:\tconv10.weight,\tTip:\ttorch.float32,\tVelicina:\t73728\n",
      "Parametar:\tconv10.bias,\tTip:\ttorch.float32,\tVelicina:\t64\n",
      "Parametar:\tconv11.weight,\tTip:\ttorch.float32,\tVelicina:\t18432\n",
      "Parametar:\tconv11.bias,\tTip:\ttorch.float32,\tVelicina:\t32\n",
      "Parametar:\tmodel_output.weight,\tTip:\ttorch.float32,\tVelicina:\t10016\n",
      "--------------- \n",
      "Ukupno parametara:\t 3901632\n"
     ]
    }
   ],
   "source": [
    "total_params=0\n",
    "for name, parameter in model.named_parameters():\n",
    "    print(f\"Parametar:\\t{name},\\tTip:\\t{parameter.dtype},\\tVelicina:\\t{parameter.numel()}\")\n",
    "    total_params+=parameter.numel()\n",
    "print(\"-\"*15,\"\\nUkupno parametara:\\t\",total_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = ReweightedCrossEntropy(Q_value=num_classes,class_weights_probs=probs, device=device)\n",
    "# for step, batch in enumerate(training, start=1):\n",
    "#     # outputs = model(**batch)\n",
    "#     # loss = outputs.loss\n",
    "#     inputs, targets = batch\n",
    "#     inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "#     # print(\"Nakon kastovanja u devajs\")\n",
    "#     outputs = model(inputs)\n",
    "#     # print(\"Nakon modela\")\n",
    "#     # print(inputs)\n",
    "#     loss = loss_fn(outputs, targets)\n",
    "#     # loss = loss_fn(outputs, torch.randint(0,251,(64,224,224),device=device))\n",
    "#     print(\"Nakon lossa\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets.long()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_entropy(outputs, torch.randint(0,251,(64,224,224),device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = cross_entropy(outputs,targets.long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_entropy(model_predictions,ground_truth,torch.as_tensor(w_value, device=self.device, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58808523c0504a1ca1f5fd354131bd31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9775 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.75 GiB (GPU 0; 8.00 GiB total capacity; 13.95 GiB already allocated; 0 bytes free; 14.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m filterwarnings(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m fit(model\u001b[39m=\u001b[39;49mmodel,optimizer\u001b[39m=\u001b[39;49moptimizer,training\u001b[39m=\u001b[39;49mtraining, validation\u001b[39m=\u001b[39;49mvalidation,scheduler\u001b[39m=\u001b[39;49mlearning_rate_scheduler,epochs\u001b[39m=\u001b[39;49mnumber_of_epochs,device\u001b[39m=\u001b[39;49mdevice, gradient_accumulation_steps\u001b[39m=\u001b[39;49m\u001b[39m12\u001b[39;49m, metric_for_early_stopping\u001b[39m=\u001b[39;49mmetric, enable_early_stopping\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,early_stopping_mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmin\u001b[39;49m\u001b[39m'\u001b[39;49m,patience\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m,epoch_start\u001b[39m=\u001b[39;49mepoch_start, loss\u001b[39m=\u001b[39;49mloss_start, shouldEvaluate\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, best\u001b[39m=\u001b[39;49mbest_result, loss_fn\u001b[39m=\u001b[39;49mReweightedCrossEntropy(Q_value\u001b[39m=\u001b[39;49mnum_classes,class_weights_probs\u001b[39m=\u001b[39;49mprobs,device\u001b[39m=\u001b[39;49mdevice))\n",
      "Cell \u001b[1;32mIn[35], line 96\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(model, optimizer, training, validation, scheduler, metric_for_early_stopping, epochs, loss_fn, gradient_accumulation_steps, enable_early_stopping, patience, early_stopping_mode, delta_for_early_stopping, best, also_use_timer_for_early_stopping, seconds_for_early_stopping, device, epoch_start, loss, shouldEvaluate)\u001b[0m\n\u001b[0;32m     94\u001b[0m losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[0;32m     95\u001b[0m loss \u001b[39m=\u001b[39m loss \u001b[39m/\u001b[39m gradient_accumulation_steps\n\u001b[1;32m---> 96\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     97\u001b[0m progress_bar\u001b[39m.\u001b[39mupdate(\u001b[39m1\u001b[39m)\n\u001b[0;32m     98\u001b[0m \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m gradient_accumulation_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32md:\\Projects\\CompVisProject\\.venv\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32md:\\Projects\\CompVisProject\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.75 GiB (GPU 0; 8.00 GiB total capacity; 13.95 GiB already allocated; 0 bytes free; 14.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "filterwarnings(\"ignore\")\n",
    "fit(model=model,optimizer=optimizer,training=training, validation=validation,scheduler=learning_rate_scheduler,epochs=number_of_epochs,device=device, gradient_accumulation_steps=12, metric_for_early_stopping=metric, enable_early_stopping=True,early_stopping_mode='min',patience=4,epoch_start=epoch_start, loss=loss_start, shouldEvaluate=False, best=best_result, loss_fn=ReweightedCrossEntropy(Q_value=num_classes,class_weights_probs=probs,device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO...\n",
    "# test = ImageDataset(dataset_memory_map[test_indices], transform=transform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorize(image: np.ndarray, print_image_inline=False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Ova metoda kolorizuje sliku koja joj je prosledjena.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.ndarray\n",
    "        Slika u RGB ili L*a*b* formatu(za sad).\n",
    "\n",
    "    print_image_inline : boolean=False\n",
    "        Da li da se slika prikaze inline ispod celije.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    \"\"\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upotreba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     output = model(torch.tensor(X).float().to(device)).cpu().numpy()\n",
    "#     output *= 128\n",
    "#     rgb_img = convert_lab_to_rgb(np.concatenate((X,output), axis=1),denormalize=True)\n",
    "#     imsave(\"img_result.png\", rgb_img[0].reshape(400,400,3))\n",
    "#     imsave(\"img_result_gray_version.png\", (255*rgb2gray(rgb_img[0].reshape(400,400,3))).astype(np.uint8))\n",
    "# model.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
