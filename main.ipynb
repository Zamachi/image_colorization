{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_model_weights=False\n",
    "dataset_path='./dataset/'\n",
    "model_weights_path = './model_weights/model_weights.pth'\n",
    "model = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\CompVisProject\\.venv\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray\n",
    "from skimage.io import imsave\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import math, time\n",
    "from tqdm import tqdm\n",
    "from lion_pytorch import Lion\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch import device\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "from gc import collect\n",
    "from os.path import isfile\n",
    "from os import remove\n",
    "from psutil import virtual_memory\n",
    "from torchvision import transforms\n",
    "_ = torch.manual_seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get images\n",
    "# image = Image.open(dataset_path+'woman.jpg').convert('RGB')\n",
    "# image = np.array(image, dtype=int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funkcija za konvertovanje slike iz sRGB u L\\*a\\*b\\*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rgb_to_lab(image: np.ndarray) -> np.ndarray: \n",
    "    \"\"\"\n",
    "    Ova funkcija konvertuje image iz sRGB prostora u L*a*b* prostor.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    image : np.NDArray, shape: Tuple[int,int,int[,int]]\n",
    "        Slika(ili slike) treba da bude 3D/4D numpy array, gde je prva dimenzija redni broj slike(ako ih ima vise), druga dimenzija broj kanala, a treca i cetvrta sirina x visina. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        Returns a `np.ndarray` of shape (num_of_images, 3, height, width) if `image` is 4D, else if `image` is 3D it returns (3, height, width).\n",
    "    \"\"\"\n",
    "    assert 3 in image.shape, f\"Nije pronadjena nijedna dimenzija koja je =3\"\n",
    "    assert 3 <= image.ndim <= 4, f\"Ocekivani broj dimenzija ulaznog parametra je izmedju 3 i 4(inclusive), a dobijeno je {image.ndim}\"\n",
    "    if image.ndim == 3:\n",
    "        assert image.shape[0] == 3, f\"Pogresna dimenzija na poziciji shape[0], ocekivano 3, dobijeno {image.shape[0]}\"\n",
    "        lab_image = rgb2lab(1.0/255*image, channel_axis=0)\n",
    "        lab_image[1:] = lab_image[1:] / 128\n",
    "        # X = lab_image[0]\n",
    "        # Y = lab_image[1:]\n",
    "        assert np.all( ( lab_image[0] >= 0) & ( lab_image[0] <= 100) ), f\"U L* kanalu pronadjeno nedozvoljenih vrednosti\"\n",
    "        assert np.all( ( lab_image[1:] >= -1) & ( lab_image[1:] <= 1) ), f\"U a*b* kanalima pronadjeno nedozvoljenih vrednosti\"\n",
    "    elif image.ndim == 4:\n",
    "        assert image.shape[1] == 3, f\"Pogresna dimenzija na poziciji shape[1], ocekivano 3, dobijeno {image.shape[1]}\"\n",
    "        lab_image = rgb2lab(1.0/255*image, channel_axis=1)\n",
    "        lab_image[:,1:] = lab_image[:,1:] / 128\n",
    "        assert np.all( (lab_image[:,0] >= 0 ) & (lab_image[:,0] <= 100 )), f\"U L* kanalu pronadjeno nedozvoljenih vrednosti\"\n",
    "        assert np.all( (lab_image[:,1:] >= -1 ) & (lab_image[:,1:] <= 1 )), f\"U a*b* kanalima pronadjeno nedozvoljenih vrednosti\"\n",
    "        # X = lab_image[:,0]\n",
    "        # Y = lab_image[:,1:]\n",
    "\n",
    "    return lab_image\n",
    "    # Y /= 128\n",
    "    # X = X.reshape(1 if image.ndim == 3 else image.shape[0], 1, image.shape[1 if image.ndim == 3 else 2], image.shape[2 if image.ndim == 3 else 3])\n",
    "    # assert np.all( (X >= 0) & (X <= 100) , axis=(1,2) if image.ndim == 3 else (2,3)).any(), f\"U L* kanalu pronadjeno nedozvoljenih vrednosti\"\n",
    "    # Y = Y.reshape(1 if image.ndim == 3 else image.shape[0], 2, image.shape[1 if image.ndim == 3 else 2], image.shape[2 if image.ndim == 3 else 3])\n",
    "    # assert np.all( (Y >= -1) & (Y <= 1) ), f\"U L* kanalu pronadjeno nedozvoljenih vrednosti\"\n",
    "    # return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: testiramo ispravnost\n",
    "# img = convert_rgb_to_lab(image.reshape(1,3,400,400))\n",
    "# X,Y = img[:,0:1,:,:], img[:,1:,:,:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funkcija za reverse convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lab_to_rgb(image: np.ndarray, denormalize=False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Ova funkcija konvertuje iz L*a*b* prostora u sRGB prostor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image: np.NDArray, shape: Tuple[int,int,int[,int]]\n",
    "        Slika(ili slike) treba da bude 3D/4D numpy array, gde je prva dimenzija redni broj slike(ako ih ima vise), druga dimenzija oznacava kanal(L*, a* ili b*), a treca i cetvrta sirina x visina. L* kanal mora da sadrzi vrednosti od 0 do 100, dok a* i b* moraju imati vrednosti izmedju -128 i 127.\n",
    "    denormalize : boolean=False\n",
    "        Da li denormalizovati podatke. Ako je denormalize=`True`, onda se koristi sRGB opseg [0,255], u protivnom se koristi [0,1]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Vraca nam sliku(ili slike) u `numpy.ndarray` formatu.   \n",
    "    \"\"\"\n",
    "    assert 3 in image.shape, f\"Nije pronadjena nijedna dimenzija koja je =3\"\n",
    "    assert 3 <= image.ndim <= 4, f\"Ocekivani broj dimenzija ulaznog parametra je izmedju 3 i 4(inclusive), a dobijeno je {image.ndim}\"\n",
    "    if image.ndim == 3:\n",
    "        assert image.shape[0] == 3, f\"Pogresna dimenzija na poziciji shape[0], ocekivano 3, dobijeno {image.shape[0]}\"\n",
    "        rgb_image = lab2rgb(image, channel_axis=0)\n",
    "    elif image.ndim == 4:\n",
    "        assert image.shape[1] == 3, f\"Pogresna dimenzija na poziciji shape[1], ocekivano 3, dobijeno {image.shape[1]}\"\n",
    "        rgb_image = lab2rgb(image, channel_axis=1)\n",
    "    rgb_image = rgb_image.reshape(1 if image.ndim == 3 else image.shape[0], 3, image.shape[1 if image.ndim == 3 else 2], image.shape[2 if image.ndim == 3 else 3])\n",
    "    assert np.all( (rgb_image >= 0) & (rgb_image <=1.0)), f\"Ocekivani opseg RGB vrednosti 0-1 je prekrsen\"\n",
    "    return rgb_image if not denormalize else (rgb_image * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: testiramo ispravnost\n",
    "# imgs_back_2rgb = convert_lab_to_rgb( np.concatenate( (X,Y), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ucitavanje i pripremu dataseta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcija za ucitavanje i kreiranje memory-mapped dataseta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(mmap_mode = None, percentage:int = 0.7, shape=(25000,3,224,224)):\n",
    "    \"\"\"\n",
    "    Treba da ucita podatke sa diska kao memory map. Memory-mapped podaci se ne ucitavaju svi u memoriju, vec se ucitavaju sa diska direktno po potrebi. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mmap_mode : str | None\n",
    "        U kom rezimu treba da ucitamo finalni dataset. Za vise videti [link](https://numpy.org/doc/stable/reference/generated/numpy.memmap.html#numpy.memmap). Ako je `None`, ucitace ceo dataset u memoriju.\n",
    "    percentage : int = 0.7\n",
    "        Koliko procenata dostupne sistemske memorije zelimo iskoristiti za konverziju.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        Ucitani L*a*b* dataset `np.ndarray`, ako je mmap_mode=`None`, u protivnom vraca `memmap` i cita se sa diska.\n",
    "    \"\"\"\n",
    "    should_delete_dataset=False\n",
    "    assert 0.1 < percentage < 0.9, f\"Ocekivan opseg procenata [10%,90%], dobijeno {percentage}\"\n",
    "    if isfile(dataset_path + 'lab_dataset.npy'):\n",
    "        return np.memmap(dataset_path + 'lab_dataset.npy',mode=mmap_mode, shape=shape,dtype=\"float16\")\n",
    "    elif isfile(dataset_path + 'joined_dataset.npy'):\n",
    "        joined_dataset = np.load(dataset_path + 'joined_dataset.npy', mmap_mode='r') # ucitavamo zdruzeni dataset\n",
    "        dataset_shape = joined_dataset.shape\n",
    "        dataset = np.memmap(dataset_path + 'lab_dataset.npy', mode=\"w+\", shape=(dataset_shape[0],3,224,224), dtype=\"float16\") # kreiramo memory-mapped fajl za finalni dataset (7 GB).\n",
    "        available_system_memory_in_GBs = virtual_memory().available/1024**3\n",
    "        how_much_memory_to_reserve_for_conversion_in_GBs = available_system_memory_in_GBs * percentage\n",
    "        converted_image_size_in_GBs = dataset_shape[0] * 8 * 3 * 224 * 224 / 1024**3  # 28 GB u sustini ako sve odjednom konvertujem.\n",
    "        print(f\"Dostupna memorija za konverziju slika {how_much_memory_to_reserve_for_conversion_in_GBs}\")\n",
    "        print(f\"Memorija potrebna za konverziju slika {converted_image_size_in_GBs}\")\n",
    "\n",
    "        try:\n",
    "            if how_much_memory_to_reserve_for_conversion_in_GBs - converted_image_size_in_GBs >= 1: # Ostavljamo 1 GB overhead-a\n",
    "                image = convert_rgb_to_lab(joined_dataset)\n",
    "                dataset[:] = image\n",
    "            else: # U protivnom koristimo batched obradu.\n",
    "                batch_size = int( (how_much_memory_to_reserve_for_conversion_in_GBs-1)*dataset_shape[0] / (32*converted_image_size_in_GBs) ) # -1 zbog memory overheada. 32/64 jer rgb2lab koristi float64...\n",
    "                print(f\"Batch size:\\t{batch_size}\")\n",
    "                assert batch_size > 1, f\"Nemate dovoljno memorije za ovakvu operaciju, ocekivano je da batch_size bude veci od 1, ali je {batch_size}\"\n",
    "                for i in range(0, dataset_shape[0], batch_size+1):\n",
    "                    dataset[i:batch_size] = convert_rgb_to_lab( joined_dataset[i:batch_size] )\n",
    "                dataset[i:] = convert_rgb_to_lab( joined_dataset[i:] )\n",
    "        except MemoryError as e:\n",
    "            print(f\"Doslo je do greske:\\n{e}\")\n",
    "            should_delete_dataset = True\n",
    "        except Exception as e:\n",
    "            should_delete_dataset = True\n",
    "            print(e)\n",
    "        finally:\n",
    "            dataset.flush()\n",
    "            print(\"Flushed!\")\n",
    "            del dataset, joined_dataset\n",
    "            collect()\n",
    "            if should_delete_dataset:\n",
    "                remove(dataset_path + 'lab_dataset.npy')\n",
    "                return\n",
    "\n",
    "        print(\"Kreirani finalni dataset\")\n",
    "        return load_data(mmap_mode, percentage, dataset_shape)\n",
    "    else:\n",
    "        X = np.load(dataset_path + 'l/gray_scale.npy',mmap_mode='r').reshape(25000,1,224,224)\n",
    "        y_file_to_load = [ f'ab/ab/ab{i}.npy' for i in range(1,4)]\n",
    "        Y = np.concatenate( [ np.load(dataset_path + file) for file in y_file_to_load ], axis=0 ).reshape(25000,2,224,224)\n",
    "        np.save(dataset_path + 'joined_dataset.npy', np.concatenate( (X,Y), axis=1))\n",
    "        del X,Y,y_file_to_load\n",
    "        collect()\n",
    "        print(\"Kreirani zdruzeni dataset\")\n",
    "        return load_data(mmap_mode, percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definisanje transformacija slika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomAffine(degrees=20, shear=0.2, scale=(0.8, 1.2)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasa naseg custom dataseta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definisanje custom dataseta\n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        \"\"\"\n",
    "        Inicijalizuje dataset\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : array-like\n",
    "            Nas dataset, oblika (broj_slika, 3, height, width).\n",
    "        transform(optional) : torchvision.Compose\n",
    "            Transformacije koje koje primenjujemo nad nasim slikama.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        image = self.data[index]\n",
    "        # image = Image.fromarray(image)\n",
    "        if self.transform:\n",
    "            print(\"be4 transform\")\n",
    "            image = self.transform(image)\n",
    "            print(\"after transform\")\n",
    "        print(image)\n",
    "        print(image.shape)\n",
    "        X = image[0]\n",
    "        Y = image[1:]\n",
    "        return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ucitavanje podataka, podela na trening i test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_memory_map = load_data('r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3, 224, 224)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_memory_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "validation_size = 0.15\n",
    "\n",
    "test_length = int(test_size * dataset_memory_map.shape[0])\n",
    "remaining_length = dataset_memory_map.shape[0] - test_length\n",
    "validation_length = int(remaining_length * validation_size)\n",
    "training_length = remaining_length - validation_length\n",
    "\n",
    "training_length, validation_length, test_length\n",
    "\n",
    "indices = [i for i in range(dataset_memory_map.shape[0])]\n",
    "\n",
    "training_indices = np.random.choice(range(0, dataset_memory_map.shape[0]), size=training_length, replace=False)\n",
    "validation_indices = np.random.choice(list(set(range(0, dataset_memory_map.shape[0])) - set(training_indices)), size=validation_length, replace=False)\n",
    "test_indices = np.random.choice(list(set(range(0, dataset_memory_map.shape[0])) - set(training_indices) - set(validation_indices)), size=test_length, replace=False)\n",
    "\n",
    "training = ImageDataset(dataset_memory_map[training_indices], transform=transform)\n",
    "validation = ImageDataset(dataset_memory_map[validation_indices], transform=transform)\n",
    "# test = ImageDataset(dataset_memory_map[test_indices], transform=transform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizerModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ColorizerModel, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1, stride=2)\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=2)\n",
    "        self.conv7 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv8 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.conv9 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.upsample1 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.conv10 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.upsample2 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.conv11 = nn.Conv2d(64, 32, kernel_size=3, padding=1)\n",
    "        self.conv12 = nn.Conv2d(32, 2, kernel_size=3, padding=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.upsample3 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.ReLU()(self.conv1(x))\n",
    "        x = nn.ReLU()(self.conv2(x))\n",
    "        x = nn.ReLU()(self.conv3(x))\n",
    "        x = nn.ReLU()(self.conv4(x))\n",
    "        x = nn.ReLU()(self.conv5(x))\n",
    "        x = nn.ReLU()(self.conv6(x))\n",
    "        x = nn.ReLU()(self.conv7(x))\n",
    "        x = nn.ReLU()(self.conv8(x))\n",
    "        x = nn.ReLU()(self.conv9(x))\n",
    "        x = self.upsample1(x)\n",
    "        x = nn.ReLU()(self.conv10(x))\n",
    "        x = self.upsample2(x)\n",
    "        x = nn.ReLU()(self.conv11(x))\n",
    "        x = self.tanh(self.conv12(x))\n",
    "        x = self.upsample3(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping(object):\n",
    "    def __init__(self, mode='min', min_delta=0, patience=10, percentage=False, also_use_timer=False, seconds_to_terminate:int=60*60):\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = 0\n",
    "        self.is_better = None\n",
    "        self.also_use_timer=also_use_timer\n",
    "        self._init_is_better(mode, min_delta, percentage)\n",
    "\n",
    "        if patience == 0:\n",
    "            self.is_better = lambda a, b: True\n",
    "            self.step = lambda a: False\n",
    "\n",
    "        if also_use_timer:\n",
    "            self.start_time=time.perf_counter()\n",
    "            self.end_time = 0\n",
    "            self.time_compare = lambda start,end: end-start >= seconds_to_terminate # NOTE Terminate after an hour\n",
    "        else:\n",
    "            self.start_time=None\n",
    "            self.end_time=None\n",
    "            self.time_compare = lambda start,end: False\n",
    "\n",
    "    def step(self, metrics):\n",
    "        if self.best is None:\n",
    "            self.best = metrics\n",
    "            return False\n",
    "\n",
    "        if torch.is_tensor(metrics):\n",
    "            if torch.isnan(metrics):\n",
    "                return True\n",
    "        elif type(metrics) == float and math.isnan(metrics):\n",
    "              return True\n",
    "\n",
    "        if self.is_better(metrics, self.best):\n",
    "            self.num_bad_epochs = 0\n",
    "            self.best = metrics\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            print('terminating because of early stopping!')\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def time_ran_out(self):\n",
    "        if self.time_compare(self.start_time, self.end_time):\n",
    "            print(\"Terminating because of training time limit.\")\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _init_is_better(self, mode, min_delta, percentage):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError('mode ' + mode + ' is unknown!')\n",
    "        if not percentage:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - min_delta\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + min_delta\n",
    "        else:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - (\n",
    "                            best * min_delta / 100)\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + (\n",
    "                            best * min_delta / 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for training and evaluating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, validation: DataLoader, device, metric, is_called_from_training=False):\n",
    "    model.eval()\n",
    "    img_real = []\n",
    "    img_pred = []\n",
    "    for step, batch in enumerate(validation):\n",
    "        with torch.no_grad():\n",
    "            inputs, targets = batch\n",
    "            if(not is_called_from_training):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            img_real.append(targets.cpu().numpy().reshape(validation.batch_size, 3, 224, 224))#NOTE: mozda puca\n",
    "            img_pred.append(outputs.cpu().numpy().reshape(validation.batch_size, 3, 224, 224))#NOTE: mozda puca\n",
    "\n",
    "    model.train()\n",
    "    metric.update(np.concatenate(img_real, axis=0), real=True)\n",
    "    metric.update(np.concatenate(img_real, axis=0), real=False)\n",
    "    return metric.compute()\n",
    "\n",
    "def fit(model: nn.Module, optimizer: optim.Optimizer, training:DataLoader, validation: DataLoader, scheduler: lr_scheduler.LRScheduler, metric_for_early_stopping,  epochs:int=50, loss_fn=nn.MSELoss(), gradient_accumulation_steps:int=8,enable_early_stopping:bool=True,patience:int=7,early_stopping_mode:str='min',delta_for_early_stopping:float=0,best:float=None,also_use_timer_for_early_stopping:bool=False, seconds_for_early_stopping:int=60*60, device:str='cpu'):\n",
    "    \"\"\"\n",
    "    Trenira/fituje model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        Ovo je objekat instanciranog modela kojeg treniramo\n",
    "    optimizer : optim.Optimizer\n",
    "        Optimizator parametara `model` koje koristimo\n",
    "    training : DataLoader\n",
    "        DataLoader za trening\n",
    "    validation : DataLoader\n",
    "        DataLoader za validaciju\n",
    "    epochs : int, optional\n",
    "        Broj epoha prilikom treninga\n",
    "    loss_fn : optional\n",
    "        Funkcija za generisanja loss-a tokom treninga `model`-a.\n",
    "    scheduler : LRSCheduler \n",
    "        Scheduler za `learning_rate` \n",
    "    gradient_accumulation_steps : int, optional\n",
    "        Koliko step-ova akumuliramo gradijente pre nego sto uradimo apdejt vejtova. Ako ne zelimo akumuliranje gradijenata, setovati ovaj parametar na 1.\n",
    "    early_stopping_mode : str, optional\n",
    "        Rezim rada early stopping mehanizma(moze biti `min` ili `max`)\n",
    "    patience : int, optional\n",
    "        Koliko koraka u EarlyStoppingu tolerisemo pre nego sto prekinemo trening\n",
    "    delta_for_early_stopping : float, optional\n",
    "        Tolerancija odstupanja performansi za early stopping\n",
    "    metric_for_early_stopping : str, optional\n",
    "        Koju metriku cemo koristiti za early stopping. \n",
    "    best : float, optional\n",
    "        Najbolji rezultat koji je model postigao. Podrazumevano nema, ako instanciramo model od 0.\n",
    "    also_use_timer_for_early_stopping : bool, optional\n",
    "        Da li se koristi i tajmer za early stopping(ako npr. zelimo da trening traje odredjeno vreme)\n",
    "    device : {'cpu', 'cuda'}\n",
    "        Na kojem uredjaju zelimo da se vrsi trening.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "\n",
    "    trainingSteps = epochs * len(training)\n",
    "    \n",
    "    if enable_early_stopping:\n",
    "        earlyStopping = EarlyStopping(patience=min(epochs, patience), mode=early_stopping_mode, min_delta=delta_for_early_stopping,also_use_timer=also_use_timer_for_early_stopping, seconds_to_terminate=seconds_for_early_stopping)\n",
    "        best = best\n",
    "\n",
    "    model.train()\n",
    "    completed_steps = 0\n",
    "    for epoch in range(epochs):\n",
    "        for step, batch in tqdm( enumerate(training, start=1), total=trainingSteps):\n",
    "            # outputs = model(**batch)\n",
    "            # loss = outputs.loss\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            # print(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            if step % gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                completed_steps += 1\n",
    "        \n",
    "        # evaluation = evaluate(model, validation, metric_for_early_stopping, is_called_from_training=True)\n",
    "        # print(f\"Tokom epohe {epoch+1} loss je bio {loss} sa akumuliranjem, tj. {loss*gradient_accumulation_steps} bez akumuliranja gradijenta, learning rate je {scheduler.get_last_lr()}\")\n",
    "        if enable_early_stopping:\n",
    "            earlyStopping.end_time = time.perf_counter()\n",
    "            # if best is None or evaluation > best:\n",
    "            #     best = evaluation[metric_for_early_stopping]\n",
    "            #     torch.save(model.state_dict(), model_weights_path) # NOTE: Mozda puca\n",
    "\n",
    "            # if earlyStopping.step(evaluation[metric_for_early_stopping]) or earlyStopping.time_ran_out():\n",
    "            #     return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asd = np.array([ img.reshape(3,400,400) for i in range(8) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, Y = asd[:,0,:,:].reshape(1,1,400,400), asd[:,1:,:,:].reshape(1,2,400,400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_epochs = 3\n",
    "batch_size = 64\n",
    "\n",
    "model = ColorizerModel()\n",
    "if use_model_weights:\n",
    "    model.load_state_dict(torch.load(model_weights_path))\n",
    "optimizer = Lion(model.parameters(), lr=3.67*0.001) \n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=5e-5)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "training = DataLoader(training, batch_size=batch_size, shuffle=True)\n",
    "# learning_rate_scheduler = lr_scheduler.LinearLR(optimizer=optimizer, start_factor=0.9,end_factor=1/5,total_iters=number_of_epochs * len(training))\n",
    "learning_rate_scheduler = lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=number_of_epochs * len(training), eta_min=0.1)\n",
    "metric = FrechetInceptionDistance(feature=64,normalize=True)\n",
    "validation = DataLoader(validation, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/798 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n",
      "(3, 224, 224)\n",
      "be4 transform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/798 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"baddbmm_with_gemm\" not implemented for 'Half'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fit(model\u001b[39m=\u001b[39;49mmodel,optimizer\u001b[39m=\u001b[39;49moptimizer,training\u001b[39m=\u001b[39;49mtraining, validation\u001b[39m=\u001b[39;49mvalidation,scheduler\u001b[39m=\u001b[39;49mlearning_rate_scheduler,epochs\u001b[39m=\u001b[39;49mnumber_of_epochs,device\u001b[39m=\u001b[39;49mdevice, gradient_accumulation_steps\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m, metric_for_early_stopping\u001b[39m=\u001b[39;49mmetric, enable_early_stopping\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "Cell \u001b[1;32mIn[16], line 67\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(model, optimizer, training, validation, scheduler, metric_for_early_stopping, epochs, loss_fn, gradient_accumulation_steps, enable_early_stopping, patience, early_stopping_mode, delta_for_early_stopping, best, also_use_timer_for_early_stopping, seconds_for_early_stopping, device)\u001b[0m\n\u001b[0;32m     65\u001b[0m completed_steps \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     66\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m---> 67\u001b[0m     \u001b[39mfor\u001b[39;00m step, batch \u001b[39min\u001b[39;00m tqdm( \u001b[39menumerate\u001b[39m(training, start\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), total\u001b[39m=\u001b[39mtrainingSteps):\n\u001b[0;32m     68\u001b[0m         \u001b[39m# outputs = model(**batch)\u001b[39;00m\n\u001b[0;32m     69\u001b[0m         \u001b[39m# loss = outputs.loss\u001b[39;00m\n\u001b[0;32m     70\u001b[0m         inputs, targets \u001b[39m=\u001b[39m batch\n\u001b[0;32m     71\u001b[0m         inputs, targets \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device), targets\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32md:\\Projects\\CompVisProject\\.venv\\lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Projects\\CompVisProject\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\Projects\\CompVisProject\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\Projects\\CompVisProject\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\Projects\\CompVisProject\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[10], line 27\u001b[0m, in \u001b[0;36mImageDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform:\n\u001b[0;32m     26\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mbe4 transform\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(image)\n\u001b[0;32m     28\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mafter transform\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[39mprint\u001b[39m(image)\n",
      "File \u001b[1;32md:\\Projects\\CompVisProject\\.venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32md:\\Projects\\CompVisProject\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Projects\\CompVisProject\\.venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:1536\u001b[0m, in \u001b[0;36mRandomAffine.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m   1532\u001b[0m img_size \u001b[39m=\u001b[39m [width, height]  \u001b[39m# flip for keeping BC on get_params call\u001b[39;00m\n\u001b[0;32m   1534\u001b[0m ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_params(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdegrees, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranslate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshear, img_size)\n\u001b[1;32m-> 1536\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49maffine(img, \u001b[39m*\u001b[39;49mret, interpolation\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation, fill\u001b[39m=\u001b[39;49mfill, center\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcenter)\n",
      "File \u001b[1;32md:\\Projects\\CompVisProject\\.venv\\lib\\site-packages\\torchvision\\transforms\\functional.py:1246\u001b[0m, in \u001b[0;36maffine\u001b[1;34m(img, angle, translate, scale, shear, interpolation, fill, center)\u001b[0m\n\u001b[0;32m   1244\u001b[0m translate_f \u001b[39m=\u001b[39m [\u001b[39m1.0\u001b[39m \u001b[39m*\u001b[39m t \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m translate]\n\u001b[0;32m   1245\u001b[0m matrix \u001b[39m=\u001b[39m _get_inverse_affine_matrix(center_f, angle, translate_f, scale, shear)\n\u001b[1;32m-> 1246\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39;49maffine(img, matrix\u001b[39m=\u001b[39;49mmatrix, interpolation\u001b[39m=\u001b[39;49minterpolation\u001b[39m.\u001b[39;49mvalue, fill\u001b[39m=\u001b[39;49mfill)\n",
      "File \u001b[1;32md:\\Projects\\CompVisProject\\.venv\\lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:617\u001b[0m, in \u001b[0;36maffine\u001b[1;34m(img, matrix, interpolation, fill)\u001b[0m\n\u001b[0;32m    615\u001b[0m shape \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mshape\n\u001b[0;32m    616\u001b[0m \u001b[39m# grid will be generated on the same device as theta and img\u001b[39;00m\n\u001b[1;32m--> 617\u001b[0m grid \u001b[39m=\u001b[39m _gen_affine_grid(theta, w\u001b[39m=\u001b[39;49mshape[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], h\u001b[39m=\u001b[39;49mshape[\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m], ow\u001b[39m=\u001b[39;49mshape[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], oh\u001b[39m=\u001b[39;49mshape[\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m])\n\u001b[0;32m    618\u001b[0m \u001b[39mreturn\u001b[39;00m _apply_grid_transform(img, grid, interpolation, fill\u001b[39m=\u001b[39mfill)\n",
      "File \u001b[1;32md:\\Projects\\CompVisProject\\.venv\\lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:601\u001b[0m, in \u001b[0;36m_gen_affine_grid\u001b[1;34m(theta, w, h, ow, oh)\u001b[0m\n\u001b[0;32m    598\u001b[0m base_grid[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mfill_(\u001b[39m1\u001b[39m)\n\u001b[0;32m    600\u001b[0m rescaled_theta \u001b[39m=\u001b[39m theta\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m) \u001b[39m/\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m w, \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m h], dtype\u001b[39m=\u001b[39mtheta\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39mtheta\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m--> 601\u001b[0m output_grid \u001b[39m=\u001b[39m base_grid\u001b[39m.\u001b[39;49mview(\u001b[39m1\u001b[39;49m, oh \u001b[39m*\u001b[39;49m ow, \u001b[39m3\u001b[39;49m)\u001b[39m.\u001b[39;49mbmm(rescaled_theta)\n\u001b[0;32m    602\u001b[0m \u001b[39mreturn\u001b[39;00m output_grid\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, oh, ow, \u001b[39m2\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: \"baddbmm_with_gemm\" not implemented for 'Half'"
     ]
    }
   ],
   "source": [
    "fit(model=model,optimizer=optimizer,training=training, validation=validation,scheduler=learning_rate_scheduler,epochs=number_of_epochs,device=device, gradient_accumulation_steps=8, metric_for_early_stopping=metric, enable_early_stopping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sth in enumerate(training):\n",
    "    print(sth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asd = convert_lab_to_rgb( np.concatenate((X,output),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asd[0,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(torch.tensor(X).float().to(device)).cpu().numpy()\n",
    "    output *= 128\n",
    "    rgb_img = convert_lab_to_rgb(np.concatenate((X,output), axis=1),denormalize=True)\n",
    "    imsave(\"img_result.png\", rgb_img[0].reshape(400,400,3))\n",
    "    imsave(\"img_result_gray_version.png\", (255*rgb2gray(rgb_img[0].reshape(400,400,3))).astype(np.uint8))\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
