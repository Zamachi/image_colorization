{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset uzet sa [link](https://www.kaggle.com/datasets/shravankumar9892/image-colorization)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path='./dataset/'\n",
    "model_weights_path = './model_weights/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray\n",
    "from skimage.io import imsave\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import math, time\n",
    "from tqdm.auto import tqdm\n",
    "from lion_pytorch import Lion\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "from gc import collect\n",
    "from os.path import isfile, exists\n",
    "from os import remove, makedirs \n",
    "from psutil import virtual_memory\n",
    "from torchvision import transforms\n",
    "from warnings import warn, filterwarnings\n",
    "import zipfile \n",
    "_ = torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not exists(dataset_path):\n",
    "    makedirs(dataset_path, exist_ok=True)\n",
    "if not exists(model_weights_path):\n",
    "    makedirs(model_weights_path, exist_ok=True)\n",
    "\n",
    "if not exists(dataset_path+'ab') and not exists(dataset_path+'l'):\n",
    "    with zipfile.ZipFile('./archive.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(dataset_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get images\n",
    "# image = Image.open(dataset_path+'woman.jpg').convert('RGB')\n",
    "# image = np.array(image, dtype=int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funkcija za konvertovanje slike iz sRGB u L\\*a\\*b\\*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rgb_to_lab(image: np.ndarray) -> np.ndarray: \n",
    "    \"\"\"\n",
    "    Ova funkcija konvertuje image iz sRGB prostora u L*a*b* prostor.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    image : np.NDArray, shape: Tuple[int,int,int[,int]]\n",
    "        Slika(ili slike) treba da bude 3D/4D numpy array, gde je prva dimenzija redni broj slike(ako ih ima vise), druga dimenzija broj kanala, a treca i cetvrta sirina x visina. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        Returns a `np.ndarray` of shape (num_of_images, 3, height, width) if `image` is 4D, else if `image` is 3D it returns (3, height, width).\n",
    "    \"\"\"\n",
    "    assert 3 in image.shape, f\"Nije pronadjena nijedna dimenzija koja je =3\"\n",
    "    assert 3 <= image.ndim <= 4, f\"Ocekivani broj dimenzija ulaznog parametra je izmedju 3 i 4(inclusive), a dobijeno je {image.ndim}\"\n",
    "    assert np.all( (image >= 0) & (image <= 255 )), f\"Ocekivano da slike budu RGB formata s opsegom [0,255], to nije dobijeno ovde...\"\n",
    "    if np.all( (image == 0) | (image == 255)):\n",
    "        warn(f\"Postoje slike koje su iskljucivo bele ili crne\")\n",
    "    if image.ndim == 3:\n",
    "        assert image.shape[0] == 3, f\"Pogresna dimenzija na poziciji shape[0], ocekivano 3, dobijeno {image.shape[0]}\"\n",
    "        lab_image = rgb2lab(1.0/255*image, channel_axis=0)\n",
    "        lab_image[1:] = lab_image[1:] / 128\n",
    "        # X = lab_image[0]\n",
    "        # Y = lab_image[1:]\n",
    "        assert np.all( ( lab_image[0] >= 0) & ( lab_image[0] <= 100) ), f\"U L* kanalu pronadjeno nedozvoljenih vrednosti\"\n",
    "        assert np.all( ( lab_image[1:] >= -1) & ( lab_image[1:] <= 1) ), f\"U a*b* kanalima pronadjeno nedozvoljenih vrednosti\"\n",
    "        if np.all( (lab_image[0] == 0.0) | (lab_image[0] == 100.0) | (lab_image[1:] == 1.0) | (lab_image[1:] == -1.0 ) | (lab_image[1:] == 0.0 )):\n",
    "            warn(f\"Potencijalno postoje slike koje su iskljucivo bele ili crne\")\n",
    "    elif image.ndim == 4:\n",
    "        assert image.shape[1] == 3, f\"Pogresna dimenzija na poziciji shape[1], ocekivano 3, dobijeno {image.shape[1]}\"\n",
    "        lab_image = rgb2lab(1.0/255*image, channel_axis=1)\n",
    "        lab_image[:,1:] = lab_image[:,1:] / 128\n",
    "        assert np.all( (lab_image[:,0:1] >= 0 ) & (lab_image[:,0:1] <= 100 )), f\"U L* kanalu pronadjeno nedozvoljenih vrednosti\"\n",
    "        assert np.all( (lab_image[:,1:] >= -1 ) & (lab_image[:,1:] <= 1 )), f\"U a*b* kanalima pronadjeno nedozvoljenih vrednosti\"\n",
    "        if np.all( (lab_image[:,0:1] == 0.0) | (lab_image[:,0:1] == 100.0) | (lab_image[:,1:] == 1.0) | (lab_image[:,1:] == -1.0 ) | (lab_image[:,1:] == 0.0 ) ):\n",
    "            warn(f\"Potencijalno postoje slike koje su iskljucivo bele ili crne\")\n",
    "        # X = lab_image[:,0]\n",
    "        # Y = lab_image[:,1:]\n",
    "    return lab_image\n",
    "    # Y /= 128\n",
    "    # X = X.reshape(1 if image.ndim == 3 else image.shape[0], 1, image.shape[1 if image.ndim == 3 else 2], image.shape[2 if image.ndim == 3 else 3])\n",
    "    # assert np.all( (X >= 0) & (X <= 100) , axis=(1,2) if image.ndim == 3 else (2,3)).any(), f\"U L* kanalu pronadjeno nedozvoljenih vrednosti\"\n",
    "    # Y = Y.reshape(1 if image.ndim == 3 else image.shape[0], 2, image.shape[1 if image.ndim == 3 else 2], image.shape[2 if image.ndim == 3 else 3])\n",
    "    # assert np.all( (Y >= -1) & (Y <= 1) ), f\"U L* kanalu pronadjeno nedozvoljenih vrednosti\"\n",
    "    # return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: testiramo ispravnost\n",
    "# img = convert_rgb_to_lab(image.reshape(1,3,400,400))\n",
    "# X,Y = img[:,0:1,:,:], img[:,1:,:,:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funkcija za reverse convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lab_to_rgb(image: np.ndarray, denormalize=False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Ova funkcija konvertuje iz L*a*b* prostora u sRGB prostor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image: np.NDArray, shape: Tuple[int,int,int[,int]]\n",
    "        Slika(ili slike) treba da bude 3D/4D numpy array, gde je prva dimenzija redni broj slike(ako ih ima vise), druga dimenzija oznacava kanal(L*, a* ili b*), a treca i cetvrta sirina x visina. L* kanal mora da sadrzi vrednosti od 0 do 100, dok a* i b* moraju imati vrednosti izmedju -128 i 127.\n",
    "    denormalize : boolean=False\n",
    "        Da li denormalizovati podatke. Ako je denormalize=`True`, onda se koristi sRGB opseg [0,255], u protivnom se koristi [0,1]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Vraca nam sliku(ili slike) u `numpy.ndarray` formatu.   \n",
    "    \"\"\"\n",
    "    assert 3 in image.shape, f\"Nije pronadjena nijedna dimenzija koja je =3\"\n",
    "    assert 3 <= image.ndim <= 4, f\"Ocekivani broj dimenzija ulaznog parametra je izmedju 3 i 4(inclusive), a dobijeno je {image.ndim}\"\n",
    "    if image.ndim == 3:\n",
    "        assert image.shape[0] == 3, f\"Pogresna dimenzija na poziciji shape[0], ocekivano 3, dobijeno {image.shape[0]}\"\n",
    "        assert np.all( (image[0] >= 0.0) & ( image[0] <= 100.0) & (image[1:] >= -128) & ( image[1:] <= 128) )\n",
    "        rgb_image = lab2rgb(image, channel_axis=0)\n",
    "    elif image.ndim == 4:\n",
    "        assert image.shape[1] == 3, f\"Pogresna dimenzija na poziciji shape[1], ocekivano 3, dobijeno {image.shape[1]}\"\n",
    "        print()\n",
    "        assert np.all( (image[:,0:1] >= 0.0) & ( image[:,0:1] <= 100.0) & (image[:,1:] >= -128) & ( image[:,1:] <= 128) )\n",
    "        rgb_image = lab2rgb(image, channel_axis=1)\n",
    "    rgb_image = rgb_image.reshape(1 if image.ndim == 3 else image.shape[0], 3, image.shape[1 if image.ndim == 3 else 2], image.shape[2 if image.ndim == 3 else 3])\n",
    "    assert np.all( (rgb_image >= 0) & (rgb_image <=1.0)), f\"Ocekivani opseg RGB vrednosti 0-1 je prekrsen\"\n",
    "    if np.all( (rgb_image == 0.0) | (rgb_image == 1.0)):\n",
    "        warn(f\"Postoje slike koje su iskljucivo crne ili bele\")\n",
    "    return rgb_image if not denormalize else (rgb_image * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: testiramo ispravnost\n",
    "# imgs_back_2rgb = convert_lab_to_rgb( np.concatenate( (X,Y), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ucitavanje i pripremu dataseta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcija za ucitavanje i kreiranje memory-mapped dataseta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(mmap_mode = None, percentage:int = 0.7, shape=(25000,3,224,224), returnJoinedInstead=False):\n",
    "    \"\"\"\n",
    "    Treba da ucita podatke sa diska kao memory map. Memory-mapped podaci se ne ucitavaju svi u memoriju, vec se ucitavaju sa diska direktno po potrebi. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mmap_mode : str | None\n",
    "        U kom rezimu treba da ucitamo finalni dataset. Za vise videti [link](https://numpy.org/doc/stable/reference/generated/numpy.memmap.html#numpy.memmap). Ako je `None`, ucitace ceo dataset u memoriju.\n",
    "    percentage : int = 0.7\n",
    "        Koliko procenata dostupne sistemske memorije zelimo iskoristiti za konverziju.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        Ucitani L*a*b* dataset `np.ndarray`, ako je mmap_mode=`None`, u protivnom vraca `memmap` i cita se sa diska.\n",
    "    \"\"\"\n",
    "    should_delete_dataset=False\n",
    "    assert 0.1 < percentage < 0.9, f\"Ocekivan opseg procenata [10%,90%], dobijeno {percentage}\"\n",
    "    if isfile(dataset_path + 'lab_dataset.npy'):\n",
    "        return np.memmap(dataset_path + 'lab_dataset.npy',mode=mmap_mode, shape=shape,dtype=\"float16\") if not returnJoinedInstead else np.load(dataset_path + 'joined_dataset.npy', mmap_mode='r')\n",
    "    elif isfile(dataset_path + 'joined_dataset.npy'):\n",
    "        joined_dataset = np.load(dataset_path + 'joined_dataset.npy', mmap_mode='r') # ucitavamo zdruzeni dataset\n",
    "        dataset_shape = joined_dataset.shape\n",
    "        dataset = np.memmap(dataset_path + 'lab_dataset.npy', mode=\"w+\", shape=(dataset_shape[0],3,224,224), dtype=\"float16\") # kreiramo memory-mapped fajl za finalni dataset (7 GB).\n",
    "        available_system_memory_in_GBs = virtual_memory().available/1024**3\n",
    "        how_much_memory_to_reserve_for_conversion_in_GBs = available_system_memory_in_GBs * percentage\n",
    "        converted_image_size_in_GBs = dataset_shape[0] * 8 * 3 * 224 * 224 / 1024**3  # 28 GB u sustini ako sve odjednom konvertujem.\n",
    "        print(f\"Dostupna memorija za konverziju slika {how_much_memory_to_reserve_for_conversion_in_GBs}\")\n",
    "        print(f\"Memorija potrebna za konverziju slika {converted_image_size_in_GBs}\")\n",
    "\n",
    "        try:\n",
    "            if how_much_memory_to_reserve_for_conversion_in_GBs - converted_image_size_in_GBs >= 1: # Ostavljamo 1 GB overhead-a\n",
    "                image = convert_rgb_to_lab(joined_dataset)\n",
    "                dataset[:] = image\n",
    "            else: # U protivnom koristimo batched obradu.\n",
    "                batch_size = int( (how_much_memory_to_reserve_for_conversion_in_GBs-1)*dataset_shape[0] / (32*converted_image_size_in_GBs) ) # -1 zbog memory overheada. 32 jer rgb2lab koristi float64...\n",
    "                print(f\"Batch size:\\t{batch_size}\")\n",
    "                assert batch_size > 1, f\"Nemate dovoljno memorije za ovakvu operaciju, ocekivano je da batch_size bude veci od 1, ali je {batch_size}\"\n",
    "                for i in range(0, dataset_shape[0], batch_size+1):\n",
    "                    dataset[i:i+batch_size] = convert_rgb_to_lab( joined_dataset[i:i+batch_size] )\n",
    "                dataset[i:] = convert_rgb_to_lab( joined_dataset[i:] )\n",
    "        except MemoryError as e:\n",
    "            print(f\"Doslo je do greske:\\n{e}\")\n",
    "            should_delete_dataset = True\n",
    "        except Exception as e:\n",
    "            should_delete_dataset = True\n",
    "            print(e)\n",
    "        finally:\n",
    "            dataset.flush()\n",
    "            print(\"Flushed!\")\n",
    "            del dataset, joined_dataset\n",
    "            collect()\n",
    "            if should_delete_dataset:\n",
    "                remove(dataset_path + 'lab_dataset.npy')\n",
    "                return\n",
    "\n",
    "        print(\"Kreirani finalni dataset\")\n",
    "        return load_data(mmap_mode, percentage, dataset_shape)\n",
    "    else:\n",
    "        X = np.load(dataset_path + 'l/gray_scale.npy',mmap_mode='r').reshape(25000,1,224,224)\n",
    "        y_file_to_load = [ f'ab/ab/ab{i}.npy' for i in range(1,4)]\n",
    "        Y = np.concatenate( [ np.load(dataset_path + file) for file in y_file_to_load ], axis=0 ).reshape(25000,2,224,224)\n",
    "        np.save(dataset_path + 'joined_dataset.npy', np.concatenate( (X,Y), axis=1))\n",
    "        del X,Y,y_file_to_load\n",
    "        collect()\n",
    "        print(\"Kreirani zdruzeni dataset\")\n",
    "        return load_data(mmap_mode, percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definisanje transformacija slika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomAffine(degrees=20, shear=0.2, scale=(0.8, 1.2)),\n",
    "    # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasa naseg custom dataseta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definisanje custom dataseta\n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data:np.ndarray, transform=None):\n",
    "        \"\"\"\n",
    "        Inicijalizuje dataset\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : array-like\n",
    "            Nas dataset, oblika (broj_slika, 3, height, width) u L*a*b* formatu.\n",
    "        transform(optional) : torchvision.Compose\n",
    "            Transformacije koje koje primenjujemo nad nasim slikama.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        image = self.data[index]\n",
    "        assert image.ndim == 3, f\"Ocekivana dimenzija slike 3, dobijena {image.ndim}\"\n",
    "        assert image.shape[0] == 3, f\"Ocekivana broj kanala slike 3, dobijeno {image.shape[0]}\"\n",
    "        assert np.all( ( image[0] >= 0.0 ) & ( image[0]  <= 100.0 )), f\"Ocekivan opseg u L* kanalu [0,100], ali red {index} u image krsi ovo pravilo.\"\n",
    "        assert np.all( ( image[1:] >= -1.0 ) & ( image[1:]  <= 1.0 )), f\"Ocekivan opseg u a* i b* kanalu [-1,1], ali red {index} u image krsi ovo pravilo.\"\n",
    "        if np.all( ( image[0:1] == -1.0 ) | ( image[0:1] == 1.0 ) | ( image[0:1] == 0.0 ) ) or np.all( (image[1:] == 0.0) | ( image[1:] == 100.0) ):\n",
    "            warn(f\"Cudne vrednosti pronadjene u slici {index}, sve vrednosti su ili 0, -1 ili 1...Istraziti...\")\n",
    "        # image = Image.fromarray(image)\n",
    "        if self.transform:\n",
    "            assert image.shape == (3,224,224), f\"Ocekivan shape slike pred obradu [3,224,224], dobijen {image.shape}\"\n",
    "            image = self.transform(torch.FloatTensor(image.reshape(3,224,224)))\n",
    "            # print(\"after transform\")\n",
    "        # image = convert_rgb_to_lab(image.reshape(3,224,224))\n",
    "        # print(image)\n",
    "        X = image[0]\n",
    "        Y = image[1:]\n",
    "        return X.reshape(-1,224,224),Y.reshape(-1,224,224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ucitavanje podataka, podela na trening i test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_memory_map = load_data('r', returnJoinedInstead=True)\n",
    "dataset_memory_map = load_data('r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: aktiviraj globalnu booleanku da testiras ovo v\n",
    "check_if_memory_map_values_valid = False \n",
    "if check_if_memory_map_values_valid:\n",
    "    use_joined=False\n",
    "    dataset_memory_map_test = load_data('r',returnJoinedInstead=use_joined)\n",
    "    for i in range(len(dataset_memory_map_test)):\n",
    "        if not use_joined:\n",
    "            assert np.all( ( dataset_memory_map_test[i][0] >= 0.0 ) & ( dataset_memory_map_test[i][0]  <= 100.0 )), f\"Ocekivan opseg u L* kanalu [0,100], ali red {i} u dataset_memory_map_test krsi ovo pravilo.\"\n",
    "            assert np.all( ( dataset_memory_map_test[i][1:] >= -1.0 ) & ( dataset_memory_map_test[i][1:]  <= 1.0 )), f\"Ocekivan opseg u a* i b* kanalu [-1,1], ali red {i} u dataset_memory_map_test krsi ovo pravilo.\"\n",
    "        else:\n",
    "            assert np.all( ( dataset_memory_map_test[i] >= 0 ) & ( dataset_memory_map_test[i] <= 255 )), f\"Ocekivan opseg u a* i b* kanalu [-1,1], ali red {i} u dataset_memory_map_test krsi ovo pravilo.\"\n",
    "        if not use_joined and ( np.all( (dataset_memory_map[i][0] == 0.0) | (dataset_memory_map[i][0] == 100.0)) or np.all( (dataset_memory_map[i][1:] == 0.0) | (dataset_memory_map[i][1:] == 1.0)| (dataset_memory_map[i][1:] == -1.0))):\n",
    "           warn(f\"Slika redni broj {i} sadrzi cudne vrednosti na L* ili a*/b* kanalima. L* treba da je [0,100](float), a a*b* [-1,1](float)\") \n",
    "        elif use_joined and np.all( (dataset_memory_map[i] == 0) | (dataset_memory_map[i] == 255) ):\n",
    "           warn(f\"Slika redni broj {i} sadrzi cudne vrednosti na RGB kanalima, opseg treba da je [0,255](uint8).\") \n",
    "\n",
    "    del dataset_memory_map_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "validation_size = 0.15\n",
    "\n",
    "test_length = int(test_size * dataset_memory_map.shape[0])\n",
    "remaining_length = dataset_memory_map.shape[0] - test_length\n",
    "validation_length = int(remaining_length * validation_size)\n",
    "training_length = remaining_length - validation_length\n",
    "\n",
    "training_length, validation_length, test_length\n",
    "\n",
    "indices = [i for i in range(dataset_memory_map.shape[0])]\n",
    "\n",
    "training_indices = np.random.choice(range(0, dataset_memory_map.shape[0]), size=training_length, replace=False)\n",
    "validation_indices = np.random.choice(list(set(range(0, dataset_memory_map.shape[0])) - set(training_indices)), size=validation_length, replace=False)\n",
    "test_indices = np.random.choice(list(set(range(0, dataset_memory_map.shape[0])) - set(training_indices) - set(validation_indices)), size=test_length, replace=False)\n",
    "\n",
    "training_ds = ImageDataset(dataset_memory_map[training_indices].copy(), transform=transform)\n",
    "validation_ds = ImageDataset(dataset_memory_map[validation_indices].copy(), transform=transform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizerModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ColorizerModel, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1, stride=2)\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=2)\n",
    "        self.conv7 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv8 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.conv9 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.upsample1 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.conv10 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.upsample2 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.conv11 = nn.Conv2d(64, 32, kernel_size=3, padding=1)\n",
    "        self.conv12 = nn.Conv2d(32, 2, kernel_size=3, padding=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.upsample3 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.ReLU()(self.conv1(x))\n",
    "        x = nn.ReLU()(self.conv2(x))\n",
    "        x = nn.ReLU()(self.conv3(x))\n",
    "        x = nn.ReLU()(self.conv4(x))\n",
    "        x = nn.ReLU()(self.conv5(x))\n",
    "        x = nn.ReLU()(self.conv6(x))\n",
    "        x = nn.ReLU()(self.conv7(x))\n",
    "        x = nn.ReLU()(self.conv8(x))\n",
    "        x = nn.ReLU()(self.conv9(x))\n",
    "        x = self.upsample1(x)\n",
    "        x = nn.ReLU()(self.conv10(x))\n",
    "        x = self.upsample2(x)\n",
    "        x = nn.ReLU()(self.conv11(x))\n",
    "        x = self.tanh(self.conv12(x))\n",
    "        x = self.upsample3(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping(object):\n",
    "    def __init__(self, mode='min', min_delta=0, patience=10, percentage=False, also_use_timer=False, seconds_to_terminate:int=60*60):\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = 0\n",
    "        self.is_better = None\n",
    "        self.also_use_timer=also_use_timer\n",
    "        self._init_is_better(mode, min_delta, percentage)\n",
    "\n",
    "        if patience == 0:\n",
    "            self.is_better = lambda a, b: True\n",
    "            self.step = lambda a: False\n",
    "\n",
    "        if also_use_timer:\n",
    "            self.start_time=time.perf_counter()\n",
    "            self.end_time = 0\n",
    "            self.time_compare = lambda start,end: end-start >= seconds_to_terminate # NOTE Terminate after an hour\n",
    "        else:\n",
    "            self.start_time=None\n",
    "            self.end_time=None\n",
    "            self.time_compare = lambda start,end: False\n",
    "\n",
    "    def step(self, metrics):\n",
    "        if self.best is None:\n",
    "            self.best = metrics\n",
    "            return False\n",
    "\n",
    "        if torch.is_tensor(metrics):\n",
    "            if torch.isnan(metrics):\n",
    "                return True\n",
    "        elif type(metrics) == float and math.isnan(metrics):\n",
    "              return True\n",
    "\n",
    "        if self.is_better(metrics, self.best):\n",
    "            self.num_bad_epochs = 0\n",
    "            self.best = metrics\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            print('terminating because of early stopping!')\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def time_ran_out(self):\n",
    "        if self.time_compare(self.start_time, self.end_time):\n",
    "            print(\"Terminating because of training time limit.\")\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _init_is_better(self, mode, min_delta, percentage):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError('mode ' + mode + ' is unknown!')\n",
    "        if not percentage:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - min_delta\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + min_delta\n",
    "        else:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - (\n",
    "                            best * min_delta / 100)\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + (\n",
    "                            best * min_delta / 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for training and evaluating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, validation: DataLoader, device, metric, is_called_from_training=False):\n",
    "    model.eval()\n",
    "    img_real_distribution = np.empty(shape=(0,3,224,224))\n",
    "    img_pred_distribution = np.empty(shape=(0,3,224,224))\n",
    "    for step, batch in enumerate(validation):\n",
    "        with torch.no_grad():\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            img_real_distribution = np.concatenate((img_real_distribution, convert_lab_to_rgb(torch.cat( (inputs,targets*128), dim=1).cpu().numpy(),denormalize=True)))\n",
    "            img_pred_distribution = np.concatenate((img_pred_distribution, convert_lab_to_rgb(torch.cat( (inputs,outputs*128), dim=1).cpu().numpy(),denormalize=True)))\n",
    "    model.train()\n",
    "    metric.update(torch.ByteTensor(img_real_distribution), real=True)\n",
    "    metric.update(torch.ByteTensor(img_pred_distribution), real=False)\n",
    "    del img_real_distribution, img_pred_distribution\n",
    "    collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return metric.compute()\n",
    "\n",
    "def fit(model: nn.Module, optimizer: optim.Optimizer, training:DataLoader, validation: DataLoader, scheduler: lr_scheduler.LRScheduler, metric_for_early_stopping,  epochs:int=50, loss_fn=nn.MSELoss(), gradient_accumulation_steps:int=8,enable_early_stopping:bool=True,patience:int=7,early_stopping_mode:str='min',delta_for_early_stopping:float=0,best:float=None,also_use_timer_for_early_stopping:bool=False, seconds_for_early_stopping:int=60*60, device:str='cpu'):\n",
    "    \"\"\"\n",
    "    Trenira/fituje model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        Ovo je objekat instanciranog modela kojeg treniramo\n",
    "    optimizer : optim.Optimizer\n",
    "        Optimizator parametara `model` koje koristimo\n",
    "    training : DataLoader\n",
    "        DataLoader za trening\n",
    "    validation : DataLoader\n",
    "        DataLoader za validaciju\n",
    "    epochs : int, optional\n",
    "        Broj epoha prilikom treninga\n",
    "    loss_fn : optional\n",
    "        Funkcija za generisanja loss-a tokom treninga `model`-a.\n",
    "    scheduler : LRSCheduler \n",
    "        Scheduler za `learning_rate` \n",
    "    gradient_accumulation_steps : int, optional\n",
    "        Koliko step-ova akumuliramo gradijente pre nego sto uradimo apdejt vejtova. Ako ne zelimo akumuliranje gradijenata, setovati ovaj parametar na 1.\n",
    "    early_stopping_mode : str, optional\n",
    "        Rezim rada early stopping mehanizma(moze biti `min` ili `max`)\n",
    "    patience : int, optional\n",
    "        Koliko koraka u EarlyStoppingu tolerisemo pre nego sto prekinemo trening\n",
    "    delta_for_early_stopping : float, optional\n",
    "        Tolerancija odstupanja performansi za early stopping\n",
    "    metric_for_early_stopping : str, optional\n",
    "        Koju metriku cemo koristiti za early stopping. \n",
    "    best : float, optional\n",
    "        Najbolji rezultat koji je model postigao. Podrazumevano nema, ako instanciramo model od 0.\n",
    "    also_use_timer_for_early_stopping : bool, optional\n",
    "        Da li se koristi i tajmer za early stopping(ako npr. zelimo da trening traje odredjeno vreme)\n",
    "    device : {'cpu', 'cuda'}\n",
    "        Na kojem uredjaju zelimo da se vrsi trening.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "\n",
    "    trainingSteps = epochs * len(training)\n",
    "    \n",
    "    if enable_early_stopping:\n",
    "        earlyStopping = EarlyStopping(patience=min(epochs, patience), mode=early_stopping_mode, min_delta=delta_for_early_stopping,also_use_timer=also_use_timer_for_early_stopping, seconds_to_terminate=seconds_for_early_stopping)\n",
    "    best = best\n",
    "    \n",
    "    progress_bar = tqdm(range(trainingSteps))\n",
    "\n",
    "    model.train()\n",
    "    completed_steps = 0\n",
    "    for epoch in range(epochs):\n",
    "        for step, batch in enumerate(training, start=1):\n",
    "            # outputs = model(**batch)\n",
    "            # loss = outputs.loss\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            # print(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            progress_bar.update(1)\n",
    "            if step % gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                completed_steps += 1\n",
    "        del batch\n",
    "        collect()\n",
    "        torch.cuda.empty_cache() \n",
    "        evaluation = evaluate(model, validation, device, metric_for_early_stopping, is_called_from_training=True)\n",
    "        # print(f\"Tokom epohe {epoch+1} loss je bio {loss} sa akumuliranjem, tj. {loss*gradient_accumulation_steps} bez akumuliranja gradijenta, learning rate je {scheduler.get_last_lr()}\")\n",
    "        earlyStopping.end_time = time.perf_counter()\n",
    "        if best is None or evaluation.item() > best:\n",
    "            best = evaluation.item()\n",
    "            torch.save(model.state_dict(), model_weights_path+'model_weights.pth') # NOTE: Mozda puca\n",
    "            torch.save(optimizer.state_dict(), model_weights_path+f'optimizer_{optimizer.__class__}_weigths.pth') # NOTE: Mozda puca\n",
    "            torch.save(scheduler.state_dict(), model_weights_path+f'scheduler_{scheduler.__class__}_weights.pth') # NOTE: Mozda puca\n",
    "\n",
    "        if enable_early_stopping and (earlyStopping.step(evaluation.item()) or earlyStopping.time_ran_out()):\n",
    "            return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_epochs = 3\n",
    "batch_size = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ColorizerModel()\n",
    "optimizer = Lion(model.parameters(), lr=3.67*0.001) \n",
    "# optimizer = torch.optim.RMSprop(model.parameters())\n",
    "# learning_rate_scheduler = lr_scheduler.LinearLR(optimizer=optimizer, start_factor=0.9,end_factor=1/5,total_iters=number_of_epochs * len(training))\n",
    "learning_rate_scheduler = lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=number_of_epochs * len(training_ds), eta_min=0.1)\n",
    "print(len(training_ds))\n",
    "use_pretrained_weights = False\n",
    "training = DataLoader(training_ds, batch_size=batch_size, shuffle=True)\n",
    "validation = DataLoader(validation_ds, batch_size=batch_size, shuffle=True)\n",
    "metric = FrechetInceptionDistance(feature=64, reset_real_features=False)\n",
    "if use_pretrained_weights:\n",
    "    model.load_state_dict(torch.load(model_weights_path+ 'model_weights.pth'))\n",
    "    optimizer.load_state_dict(torch.load(model_weights_path + f'optimizer_{optimizer.__class__}_weights.pth'))\n",
    "    learning_rate_scheduler.load_state_dict(torch.load(model_weights_path + f'lr_scheduler_{optimizer.__class__}_weights.pth'))\n",
    "\n",
    "del training_ds, validation_ds\n",
    "collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.to(device)\n",
    "# model.eval()\n",
    "# img_real_distribution = []\n",
    "# img_pred_distribution = []\n",
    "# for step, batch in enumerate(validation):\n",
    "#     with torch.no_grad():\n",
    "#         inputs, targets = batch\n",
    "#         if(not False):\n",
    "#             inputs, targets = inputs.to(device), targets.to(device)\n",
    "#         outputs = model(inputs)\n",
    "#         img_real_distribution.append(convert_lab_to_rgb(torch.cat( (inputs,targets*128), dim=1).cpu().numpy(),denormalize=True))#NOTE: mozda puca\n",
    "#         img_pred_distribution.append(convert_lab_to_rgb(torch.cat( (inputs,outputs*128), dim=1).cpu().numpy(),denormalize=True))#NOTE: mozda puca\n",
    "#     break\n",
    "# model.train()\n",
    "# metric.update(torch.ByteTensor(test_real), real=True)\n",
    "# metric.update(torch.ByteTensor(test_pred), real=False)\n",
    "# metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filterwarnings(\"ignore\")\n",
    "# model.to(device)\n",
    "# test_real, test_pred = evaluate(model,validation,device,metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Struktura i parametri modela prikazani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params=0\n",
    "for name, parameter in model.named_parameters():\n",
    "    print(f\"Parametar:\\t{name},\\tTip:\\t{parameter.dtype},\\tVelicina:\\t{parameter.numel()}\")\n",
    "    total_params+=parameter.numel()\n",
    "print(\"-\"*15,\"\\nUkupno parametara:\\t\",total_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 3218436000 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m filterwarnings(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m fit(model\u001b[39m=\u001b[39;49mmodel,optimizer\u001b[39m=\u001b[39;49moptimizer,training\u001b[39m=\u001b[39;49mtraining, validation\u001b[39m=\u001b[39;49mvalidation,scheduler\u001b[39m=\u001b[39;49mlearning_rate_scheduler,epochs\u001b[39m=\u001b[39;49mnumber_of_epochs,device\u001b[39m=\u001b[39;49mdevice, gradient_accumulation_steps\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m, metric_for_early_stopping\u001b[39m=\u001b[39;49mmetric, enable_early_stopping\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "Cell \u001b[1;32mIn[16], line 89\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(model, optimizer, training, validation, scheduler, metric_for_early_stopping, epochs, loss_fn, gradient_accumulation_steps, enable_early_stopping, patience, early_stopping_mode, delta_for_early_stopping, best, also_use_timer_for_early_stopping, seconds_for_early_stopping, device)\u001b[0m\n\u001b[0;32m     87\u001b[0m collect()\n\u001b[0;32m     88\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache() \n\u001b[1;32m---> 89\u001b[0m evaluation \u001b[39m=\u001b[39m evaluate(model, validation, device, metric_for_early_stopping, is_called_from_training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     90\u001b[0m \u001b[39m# print(f\"Tokom epohe {epoch+1} loss je bio {loss} sa akumuliranjem, tj. {loss*gradient_accumulation_steps} bez akumuliranja gradijenta, learning rate je {scheduler.get_last_lr()}\")\u001b[39;00m\n\u001b[0;32m     91\u001b[0m earlyStopping\u001b[39m.\u001b[39mend_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n",
      "Cell \u001b[1;32mIn[16], line 13\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, validation, device, metric, is_called_from_training)\u001b[0m\n\u001b[0;32m     11\u001b[0m         img_pred_distribution \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((img_pred_distribution, convert_lab_to_rgb(torch\u001b[39m.\u001b[39mcat( (inputs,outputs\u001b[39m*\u001b[39m\u001b[39m128\u001b[39m), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy(),denormalize\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)))\n\u001b[0;32m     12\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m---> 13\u001b[0m metric\u001b[39m.\u001b[39;49mupdate(torch\u001b[39m.\u001b[39;49mByteTensor(img_real_distribution), real\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     14\u001b[0m metric\u001b[39m.\u001b[39mupdate(torch\u001b[39m.\u001b[39mByteTensor(img_pred_distribution), real\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m \u001b[39mdel\u001b[39;00m img_real_distribution, img_pred_distribution\n",
      "File \u001b[1;32md:\\Projects\\CompVisProject\\.venv\\lib\\site-packages\\torchmetrics\\metric.py:456\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mExpected all tensors to be on\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(err):\n\u001b[0;32m    449\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    450\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mEncountered different devices in metric calculation (see stacktrace for details).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    451\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m This could be due to the metric class not being on the same device as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    454\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m device corresponds to the device of the input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    455\u001b[0m             ) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m         \u001b[39mraise\u001b[39;00m err\n\u001b[0;32m    458\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_on_cpu:\n\u001b[0;32m    459\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_move_list_states_to_cpu()\n",
      "File \u001b[1;32md:\\Projects\\CompVisProject\\.venv\\lib\\site-packages\\torchmetrics\\metric.py:446\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_grad):\n\u001b[0;32m    445\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 446\u001b[0m         update(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    447\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    448\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mExpected all tensors to be on\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(err):\n",
      "File \u001b[1;32md:\\Projects\\CompVisProject\\.venv\\lib\\site-packages\\torchmetrics\\image\\fid.py:328\u001b[0m, in \u001b[0;36mFrechetInceptionDistance.update\u001b[1;34m(self, imgs, real)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Update the state with extracted features.\"\"\"\u001b[39;00m\n\u001b[0;32m    327\u001b[0m imgs \u001b[39m=\u001b[39m (imgs \u001b[39m*\u001b[39m \u001b[39m255\u001b[39m)\u001b[39m.\u001b[39mbyte() \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize \u001b[39melse\u001b[39;00m imgs\n\u001b[1;32m--> 328\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minception(imgs)\n\u001b[0;32m    329\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morig_dtype \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mdtype\n\u001b[0;32m    330\u001b[0m features \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mdouble()\n",
      "File \u001b[1;32md:\\Projects\\CompVisProject\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Projects\\CompVisProject\\.venv\\lib\\site-packages\\torchmetrics\\image\\fid.py:156\u001b[0m, in \u001b[0;36mNoTrainInceptionV3.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m    155\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Forward pass of neural network with reshaping of output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_torch_fidelity_forward(x)\n\u001b[0;32m    157\u001b[0m     \u001b[39mreturn\u001b[39;00m out[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mreshape(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32md:\\Projects\\CompVisProject\\.venv\\lib\\site-packages\\torchmetrics\\image\\fid.py:84\u001b[0m, in \u001b[0;36mNoTrainInceptionV3._torch_fidelity_forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     81\u001b[0m remaining_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures_list\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m     83\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dtype) \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_dtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat)\n\u001b[1;32m---> 84\u001b[0m x \u001b[39m=\u001b[39m interpolate_bilinear_2d_like_tensorflow1x(\n\u001b[0;32m     85\u001b[0m     x,\n\u001b[0;32m     86\u001b[0m     size\u001b[39m=\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mINPUT_IMAGE_SIZE, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mINPUT_IMAGE_SIZE),\n\u001b[0;32m     87\u001b[0m     align_corners\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     88\u001b[0m )\n\u001b[0;32m     89\u001b[0m x \u001b[39m=\u001b[39m (x \u001b[39m-\u001b[39m \u001b[39m128\u001b[39m) \u001b[39m/\u001b[39m \u001b[39m128\u001b[39m\n\u001b[0;32m     91\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mConv2d_1a_3x3(x)\n",
      "File \u001b[1;32md:\\Projects\\CompVisProject\\.venv\\lib\\site-packages\\torch_fidelity\\interpolate_compat_tensorflow.py:139\u001b[0m, in \u001b[0;36minterpolate_bilinear_2d_like_tensorflow1x\u001b[1;34m(input, size, scale_factor, align_corners, method)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n\u001b[0;32m    138\u001b[0m \u001b[39mif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mslow\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m     out \u001b[39m=\u001b[39m resample_manually()\n\u001b[0;32m    140\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    141\u001b[0m     out \u001b[39m=\u001b[39m resample_using_grid_sample()\n",
      "File \u001b[1;32md:\\Projects\\CompVisProject\\.venv\\lib\\site-packages\\torch_fidelity\\interpolate_compat_tensorflow.py:129\u001b[0m, in \u001b[0;36minterpolate_bilinear_2d_like_tensorflow1x.<locals>.resample_manually\u001b[1;34m()\u001b[0m\n\u001b[0;32m    127\u001b[0m in_00 \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m[:, :, grid_y_lo, :][:, :, :, grid_x_lo]\n\u001b[0;32m    128\u001b[0m in_01 \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m[:, :, grid_y_lo, :][:, :, :, grid_x_hi]\n\u001b[1;32m--> 129\u001b[0m in_10 \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m[:, :, grid_y_hi, :][:, :, :, grid_x_lo]\n\u001b[0;32m    130\u001b[0m in_11 \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m[:, :, grid_y_hi, :][:, :, :, grid_x_hi]\n\u001b[0;32m    132\u001b[0m in_0 \u001b[39m=\u001b[39m in_00 \u001b[39m+\u001b[39m (in_01 \u001b[39m-\u001b[39m in_00) \u001b[39m*\u001b[39m grid_dx\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, out_size[\u001b[39m1\u001b[39m])\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 3218436000 bytes."
     ]
    }
   ],
   "source": [
    "filterwarnings(\"ignore\")\n",
    "fit(model=model,optimizer=optimizer,training=training, validation=validation,scheduler=learning_rate_scheduler,epochs=number_of_epochs,device=device, gradient_accumulation_steps=8, metric_for_early_stopping=metric, enable_early_stopping=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO...\n",
    "# test = ImageDataset(dataset_memory_map[test_indices], transform=transform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorize(image: np.ndarray, print_image_inline=False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Ova metoda kolorizuje sliku koja joj je prosledjena.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.ndarray\n",
    "        Slika u RGB ili L*a*b* formatu(za sad).\n",
    "\n",
    "    print_image_inline : boolean=False\n",
    "        Da li da se slika prikaze inline ispod celije.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    \"\"\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upotreba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     output = model(torch.tensor(X).float().to(device)).cpu().numpy()\n",
    "#     output *= 128\n",
    "#     rgb_img = convert_lab_to_rgb(np.concatenate((X,output), axis=1),denormalize=True)\n",
    "#     imsave(\"img_result.png\", rgb_img[0].reshape(400,400,3))\n",
    "#     imsave(\"img_result_gray_version.png\", (255*rgb2gray(rgb_img[0].reshape(400,400,3))).astype(np.uint8))\n",
    "# model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
