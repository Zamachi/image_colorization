{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset uzet sa [link](https://www.kaggle.com/datasets/shravankumar9892/image-colorization)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  from google.colab import output\n",
    "  output.enable_custom_widget_manager()\n",
    "  import os\n",
    "  os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "IN_COLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    %pip install torchvision torchaudio torch scipy numpy scikit-image lion-pytorch torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path='./dataset/'\n",
    "model_weights_path = './model_weights/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\CompVisProject\\.venv\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray\n",
    "from skimage.io import imsave\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import math, time\n",
    "from tqdm.auto import tqdm\n",
    "from lion_pytorch import Lion\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from gc import collect\n",
    "from os.path import isfile, exists\n",
    "from os import remove, makedirs \n",
    "from psutil import virtual_memory\n",
    "from torchvision import transforms\n",
    "from warnings import warn, filterwarnings\n",
    "import zipfile \n",
    "from itertools import islice\n",
    "# import kaggle.api\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "seed=42\n",
    "_ = torch.manual_seed(seed)\n",
    "_np = np.random.seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    _cuda = torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e405d83c93d6418ea264c4f981b77ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('imagenet-1k', 'en', split='train', streaming=True)\n",
    "num_classes=22\n",
    "bins = np.linspace(-128, 128, num=num_classes)\n",
    "distros = np.memmap(f'{dataset_path}distributions.npy',mode=\"w+\",dtype=np.ulonglong,shape=(num_classes ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = dict()\n",
    "\n",
    "class_num = 0\n",
    "for i in range(num_classes):\n",
    "  for j in range(num_classes):\n",
    "    mappings[str(j) + str(i)] = class_num\n",
    "    class_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gde_sam_stao=int(1281167/2)\n",
    "brojac=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sth in dataset:\n",
    "  if brojac != gde_sam_stao:\n",
    "    brojac+=1\n",
    "    continue\n",
    "  if np.array(sth['image']).ndim == 3:\n",
    "    lab_image = rgb2lab( (np.array(sth['image'].resize((224,224))).reshape(224,224,3))/255, channel_axis=2)\n",
    "    quantized_a = np.digitize(lab_image[:,:,1], bins=bins)\n",
    "    quantized_b = np.digitize(lab_image[:,:,2], bins=bins)\n",
    "    for i in range(224):\n",
    "      for j in range(224):\n",
    "        distros[mappings[str(quantized_a[i,j]) + str(quantized_b[i,j])]] += 1\n",
    "        brojac+=1\n",
    "        gde_sam_stao = brojac\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188394"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brojac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "distros.flush()\n",
    "del distros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "distros = np.memmap(f'{dataset_path}distributions.npy',dtype=np.ulonglong, mode=\"r+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(484,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distros.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isfile('./archive.zip'):\n",
    "    kaggle.api.authenticate()\n",
    "    kaggle.api.dataset_download_files('shravankumar9892/image-colorization',path='.',unzip=False)\n",
    "\n",
    "if not exists(dataset_path):\n",
    "    makedirs(dataset_path, exist_ok=True)\n",
    "if not exists(model_weights_path):\n",
    "    makedirs(model_weights_path, exist_ok=True)\n",
    "\n",
    "if not exists(dataset_path+'ab') and not exists(dataset_path+'l'):\n",
    "    with zipfile.ZipFile('./archive.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(dataset_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get images\n",
    "# image = Image.open(dataset_path+'woman.jpg').convert('RGB')\n",
    "# image = np.array(image, dtype=int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funkcija za konvertovanje slike iz sRGB u L\\*a\\*b\\*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rgb_to_lab(image: np.ndarray) -> np.ndarray: \n",
    "    \"\"\"\n",
    "    Ova funkcija konvertuje image iz sRGB prostora u L*a*b* prostor.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    image : np.NDArray, shape: Tuple[int,int,int[,int]]\n",
    "        Slika(ili slike) treba da bude 3D/4D numpy array, gde je prva dimenzija redni broj slike(ako ih ima vise), druga dimenzija broj kanala, a treca i cetvrta sirina x visina. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        Returns a `np.ndarray` of shape (num_of_images, 3, height, width) if `image` is 4D, else if `image` is 3D it returns (3, height, width).\n",
    "    \"\"\"\n",
    "    assert 3 in image.shape, f\"Nije pronadjena nijedna dimenzija koja je =3\"\n",
    "    assert 3 <= image.ndim <= 4, f\"Ocekivani broj dimenzija ulaznog parametra je izmedju 3 i 4(inclusive), a dobijeno je {image.ndim}\"\n",
    "    assert np.all( (image >= 0) & (image <= 255 )), f\"Ocekivano da slike budu RGB formata s opsegom [0,255], to nije dobijeno ovde...\"\n",
    "    if np.all( (image == 0) | (image == 255)):\n",
    "        warn(f\"Postoje slike koje su iskljucivo bele ili crne\")\n",
    "    if image.ndim == 3:\n",
    "        assert image.shape[0] == 3, f\"Pogresna dimenzija na poziciji shape[0], ocekivano 3, dobijeno {image.shape[0]}\"\n",
    "        lab_image = rgb2lab(1.0/255*image, channel_axis=0)\n",
    "        lab_image[1:] = lab_image[1:] / 128\n",
    "        # X = lab_image[0]\n",
    "        # Y = lab_image[1:]\n",
    "        assert np.all( ( lab_image[0] >= 0) & ( lab_image[0] <= 100) ), f\"U L* kanalu pronadjeno nedozvoljenih vrednosti\"\n",
    "        assert np.all( ( lab_image[1:] >= -1) & ( lab_image[1:] <= 1) ), f\"U a*b* kanalima pronadjeno nedozvoljenih vrednosti\"\n",
    "        if np.all( (lab_image[0] == 0.0) | (lab_image[0] == 100.0) | (lab_image[1:] == 1.0) | (lab_image[1:] == -1.0 ) | (lab_image[1:] == 0.0 )):\n",
    "            warn(f\"Potencijalno postoje slike koje su iskljucivo bele ili crne\")\n",
    "    elif image.ndim == 4:\n",
    "        assert image.shape[1] == 3, f\"Pogresna dimenzija na poziciji shape[1], ocekivano 3, dobijeno {image.shape[1]}\"\n",
    "        lab_image = rgb2lab(1.0/255*image, channel_axis=1)\n",
    "        lab_image[:,1:] = lab_image[:,1:] / 128\n",
    "        assert np.all( (lab_image[:,0:1] >= 0 ) & (lab_image[:,0:1] <= 100 )), f\"U L* kanalu pronadjeno nedozvoljenih vrednosti\"\n",
    "        assert np.all( (lab_image[:,1:] >= -1 ) & (lab_image[:,1:] <= 1 )), f\"U a*b* kanalima pronadjeno nedozvoljenih vrednosti\"\n",
    "        if np.all( (lab_image[:,0:1] == 0.0) | (lab_image[:,0:1] == 100.0) | (lab_image[:,1:] == 1.0) | (lab_image[:,1:] == -1.0 ) | (lab_image[:,1:] == 0.0 ) ):\n",
    "            warn(f\"Potencijalno postoje slike koje su iskljucivo bele ili crne\")\n",
    "        # X = lab_image[:,0]\n",
    "        # Y = lab_image[:,1:]\n",
    "    return lab_image\n",
    "    # Y /= 128\n",
    "    # X = X.reshape(1 if image.ndim == 3 else image.shape[0], 1, image.shape[1 if image.ndim == 3 else 2], image.shape[2 if image.ndim == 3 else 3])\n",
    "    # assert np.all( (X >= 0) & (X <= 100) , axis=(1,2) if image.ndim == 3 else (2,3)).any(), f\"U L* kanalu pronadjeno nedozvoljenih vrednosti\"\n",
    "    # Y = Y.reshape(1 if image.ndim == 3 else image.shape[0], 2, image.shape[1 if image.ndim == 3 else 2], image.shape[2 if image.ndim == 3 else 3])\n",
    "    # assert np.all( (Y >= -1) & (Y <= 1) ), f\"U L* kanalu pronadjeno nedozvoljenih vrednosti\"\n",
    "    # return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: testiramo ispravnost\n",
    "# img = convert_rgb_to_lab(image.reshape(1,3,400,400))\n",
    "# X,Y = img[:,0:1,:,:], img[:,1:,:,:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funkcija za reverse convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lab_to_rgb(image: np.ndarray, denormalize=False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Ova funkcija konvertuje iz L*a*b* prostora u sRGB prostor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image: np.NDArray, shape: Tuple[int,int,int[,int]]\n",
    "        Slika(ili slike) treba da bude 3D/4D numpy array, gde je prva dimenzija redni broj slike(ako ih ima vise), druga dimenzija oznacava kanal(L*, a* ili b*), a treca i cetvrta sirina x visina. L* kanal mora da sadrzi vrednosti od 0 do 100, dok a* i b* moraju imati vrednosti izmedju -128 i 127.\n",
    "    denormalize : boolean=False\n",
    "        Da li denormalizovati podatke. Ako je denormalize=`True`, onda se koristi sRGB opseg [0,255], u protivnom se koristi [0,1]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Vraca nam sliku(ili slike) u `numpy.ndarray` formatu.   \n",
    "    \"\"\"\n",
    "    assert 3 in image.shape, f\"Nije pronadjena nijedna dimenzija koja je =3\"\n",
    "    assert 3 <= image.ndim <= 4, f\"Ocekivani broj dimenzija ulaznog parametra je izmedju 3 i 4(inclusive), a dobijeno je {image.ndim}\"\n",
    "    if image.ndim == 3:\n",
    "        assert image.shape[0] == 3, f\"Pogresna dimenzija na poziciji shape[0], ocekivano 3, dobijeno {image.shape[0]}\"\n",
    "        assert np.all( (image[0] >= 0.0) & ( image[0] <= 100.0) & (image[1:] >= -128) & ( image[1:] <= 128) )\n",
    "        rgb_image = lab2rgb(image, channel_axis=0)\n",
    "    elif image.ndim == 4:\n",
    "        assert image.shape[1] == 3, f\"Pogresna dimenzija na poziciji shape[1], ocekivano 3, dobijeno {image.shape[1]}\"\n",
    "        print()\n",
    "        assert np.all( (image[:,0:1] >= 0.0) & ( image[:,0:1] <= 100.0) & (image[:,1:] >= -128) & ( image[:,1:] <= 128) )\n",
    "        rgb_image = lab2rgb(image, channel_axis=1)\n",
    "    # rgb_image = rgb_image.reshape(1 if image.ndim == 3 else image.shape[0], 3, image.shape[1 if image.ndim == 3 else 2], image.shape[2 if image.ndim == 3 else 3])\n",
    "    assert np.all( (rgb_image >= 0) & (rgb_image <=1.0)), f\"Ocekivani opseg RGB vrednosti 0-1 je prekrsen\"\n",
    "    if np.all( (rgb_image == 0.0) | (rgb_image == 1.0)):\n",
    "        warn(f\"Postoje slike koje su iskljucivo crne ili bele\")\n",
    "    return rgb_image if not denormalize else (rgb_image * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: testiramo ispravnost\n",
    "# imgs_back_2rgb = convert_lab_to_rgb( np.concatenate( (X,Y), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ucitavanje i pripremu dataseta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcija za ucitavanje i kreiranje memory-mapped dataseta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(mmap_mode = None, percentage:int = 0.7, shape=(25000,3,224,224), returnJoinedInstead=False):\n",
    "    \"\"\"\n",
    "    Treba da ucita podatke sa diska kao memory map. Memory-mapped podaci se ne ucitavaju svi u memoriju, vec se ucitavaju sa diska direktno po potrebi. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mmap_mode : str | None\n",
    "        U kom rezimu treba da ucitamo finalni dataset. Za vise videti [link](https://numpy.org/doc/stable/reference/generated/numpy.memmap.html#numpy.memmap). Ako je `None`, ucitace ceo dataset u memoriju.\n",
    "    percentage : int = 0.7\n",
    "        Koliko procenata dostupne sistemske memorije zelimo iskoristiti za konverziju.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        Ucitani L*a*b* dataset `np.ndarray`, ako je mmap_mode=`None`, u protivnom vraca `memmap` i cita se sa diska.\n",
    "    \"\"\"\n",
    "    should_delete_dataset=False\n",
    "    assert 0.1 < percentage < 0.9, f\"Ocekivan opseg procenata [10%,90%], dobijeno {percentage}\"\n",
    "    if isfile(dataset_path + 'lab_dataset.npy'):\n",
    "        return np.memmap(dataset_path + 'lab_dataset.npy',mode=mmap_mode, shape=shape,dtype=\"float16\") if not returnJoinedInstead else np.load(dataset_path + 'joined_dataset.npy', mmap_mode='r')\n",
    "    elif isfile(dataset_path + 'joined_dataset.npy'):\n",
    "        joined_dataset = np.load(dataset_path + 'joined_dataset.npy', mmap_mode='r') # ucitavamo zdruzeni dataset\n",
    "        dataset_shape = joined_dataset.shape\n",
    "        dataset = np.memmap(dataset_path + 'lab_dataset.npy', mode=\"w+\", shape=(dataset_shape[0],3,224,224), dtype=\"float16\") # kreiramo memory-mapped fajl za finalni dataset (7 GB).\n",
    "        available_system_memory_in_GBs = virtual_memory().available/1024**3\n",
    "        how_much_memory_to_reserve_for_conversion_in_GBs = available_system_memory_in_GBs * percentage\n",
    "        converted_image_size_in_GBs = dataset_shape[0] * 8 * 3 * 224 * 224 / 1024**3  # 28 GB u sustini ako sve odjednom konvertujem.\n",
    "        print(f\"Dostupna memorija za konverziju slika {how_much_memory_to_reserve_for_conversion_in_GBs}\")\n",
    "        print(f\"Memorija potrebna za konverziju slika {converted_image_size_in_GBs}\")\n",
    "\n",
    "        try:\n",
    "            if how_much_memory_to_reserve_for_conversion_in_GBs - converted_image_size_in_GBs >= 1: # Ostavljamo 1 GB overhead-a\n",
    "                image = convert_rgb_to_lab(joined_dataset)\n",
    "                dataset[:] = image\n",
    "            else: # U protivnom koristimo batched obradu.\n",
    "                batch_size = int( (how_much_memory_to_reserve_for_conversion_in_GBs-1)*dataset_shape[0] / (32*converted_image_size_in_GBs) ) # -1 zbog memory overheada. 32 jer rgb2lab koristi float64...\n",
    "                print(f\"Batch size:\\t{batch_size}\")\n",
    "                assert batch_size > 1, f\"Nemate dovoljno memorije za ovakvu operaciju, ocekivano je da batch_size bude veci od 1, ali je {batch_size}\"\n",
    "                for i in range(0, dataset_shape[0], batch_size+1):\n",
    "                    dataset[i:i+batch_size] = convert_rgb_to_lab( joined_dataset[i:i+batch_size] )\n",
    "                dataset[i:] = convert_rgb_to_lab( joined_dataset[i:] )\n",
    "        except MemoryError as e:\n",
    "            print(f\"Doslo je do greske:\\n{e}\")\n",
    "            should_delete_dataset = True\n",
    "        except Exception as e:\n",
    "            should_delete_dataset = True\n",
    "            print(e)\n",
    "        finally:\n",
    "            dataset.flush()\n",
    "            print(\"Flushed!\")\n",
    "            del dataset, joined_dataset\n",
    "            collect()\n",
    "            if should_delete_dataset:\n",
    "                remove(dataset_path + 'lab_dataset.npy')\n",
    "                return\n",
    "\n",
    "        print(\"Kreirani finalni dataset\")\n",
    "        return load_data(mmap_mode, percentage, dataset_shape)\n",
    "    else:\n",
    "        X = np.load(dataset_path + 'l/gray_scale.npy',mmap_mode='r').reshape(25000,1,224,224)\n",
    "        y_file_to_load = [ f'ab/ab/ab{i}.npy' for i in range(1,4)]\n",
    "        Y = np.concatenate( [ np.load(dataset_path + file) for file in y_file_to_load ], axis=0 ).reshape(25000,2,224,224)\n",
    "        np.save(dataset_path + 'joined_dataset.npy', np.concatenate( (X,Y), axis=1))\n",
    "        del X,Y,y_file_to_load\n",
    "        collect()\n",
    "        print(\"Kreirani zdruzeni dataset\")\n",
    "        return load_data(mmap_mode, percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definisanje transformacija slika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: transformacija slike nije podrzana...\n",
    "# transform = transforms.Compose([\n",
    "#     # transforms.ToTensor(),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomAffine(degrees=20, shear=0.2, scale=(0.8, 1.2)),\n",
    "#     # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasa naseg custom dataseta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definisanje custom dataseta\n",
    "class IterableImageDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, dataset_indices: list, return_joined:bool=False, transform=None):\n",
    "        \"\"\"\n",
    "        Inicijalizuje dataset\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_indices : list\n",
    "            Koje redove iz dataseta koristimo\n",
    "        transform(optional) : torchvision.Compose\n",
    "            Transformacije koje koje primenjujemo nad nasim slikama.\n",
    "        \"\"\"\n",
    "        # assert data.ndim == 4, f\"Ocekivan broj dimenzija dataseta 4, dobijeno {data.ndim}\"\n",
    "        # assert data.shape == (data.shape[0],3,224,224), f\"Ocekivan shape dataseta (25000,3,224,224), dobijeno {data.shape}\"\n",
    "        # assert np.all( (data >= 0) & (data <= 255)), f\"Slike treba da su u RGB formatu!\"\n",
    "        self.data = load_data(\"r\", returnJoinedInstead=return_joined) \n",
    "        self.is_joined = return_joined\n",
    "        self.indices = dataset_indices\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def process_data(self, dataset):\n",
    "        for idx in self.indices:\n",
    "            img = dataset[idx]\n",
    "            if self.transform:\n",
    "                img_transformed = self.transform(torch.as_tensor( convert_lab_to_rgb(img) if not self.is_joined else img,dtype=torch.float32))\n",
    "                if self.is_joined:\n",
    "                    img_transformed = convert_rgb_to_lab(img_transformed)\n",
    "\n",
    "                #NOTE ovde konvertovati ab kanale u Q_values da bude (batch-size,Q_values,width,height)\n",
    "                yield img_transformed[0].reshape(-1,224,224), img_transformed[1:].reshape(-1,224,224)\n",
    "            else:\n",
    "                if self.is_joined:\n",
    "                    img = convert_rgb_to_lab(img)\n",
    "                yield img[0].reshape(-1,224,224), img[1:].reshape(-1,224,224)\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info() # NOTE ako je None, onda je single-process\n",
    "        if worker_info is None:\n",
    "            return iter(self.process_data(self.data))\n",
    "            # return islice(map(self.process_data, iter(self.data)),0)\n",
    "        total_number_of_workers = worker_info.num_workers\n",
    "        worker_id = worker_info.id\n",
    "        dataset_length = self.__len__()\n",
    "\n",
    "        mapped_iterator = map(self.process_data, iter(self.data))\n",
    "        # return islice(iter(self.process_data(self.data[self.indices])), worker_id, None, total_number_of_workers)\n",
    "        return islice(mapped_iterator, worker_id, None, total_number_of_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ucitavanje podataka, podela na trening i test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_memory_map = load_data('r')\n",
    "# dataset_memory_map = load_data('r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: aktiviraj globalnu booleanku da testiras ovo v\n",
    "check_if_memory_map_values_valid = False \n",
    "if check_if_memory_map_values_valid:\n",
    "    use_joined=False\n",
    "    dataset_memory_map_test = load_data('r',returnJoinedInstead=use_joined)\n",
    "    for i in range(len(dataset_memory_map_test)):\n",
    "        if not use_joined:\n",
    "            assert np.all( ( dataset_memory_map_test[i][0] >= 0.0 ) & ( dataset_memory_map_test[i][0]  <= 100.0 )), f\"Ocekivan opseg u L* kanalu [0,100], ali red {i} u dataset_memory_map_test krsi ovo pravilo.\"\n",
    "            assert np.all( ( dataset_memory_map_test[i][1:] >= -1.0 ) & ( dataset_memory_map_test[i][1:]  <= 1.0 )), f\"Ocekivan opseg u a* i b* kanalu [-1,1], ali red {i} u dataset_memory_map_test krsi ovo pravilo.\"\n",
    "        else:\n",
    "            assert np.all( ( dataset_memory_map_test[i] >= 0 ) & ( dataset_memory_map_test[i] <= 255 )), f\"Ocekivan opseg u a* i b* kanalu [-1,1], ali red {i} u dataset_memory_map_test krsi ovo pravilo.\"\n",
    "        if not use_joined and ( np.all( (dataset_memory_map[i][0] == 0.0) | (dataset_memory_map[i][0] == 100.0)) or np.all( (dataset_memory_map[i][1:] == 0.0) | (dataset_memory_map[i][1:] == 1.0)| (dataset_memory_map[i][1:] == -1.0))):\n",
    "           warn(f\"Slika redni broj {i} sadrzi cudne vrednosti na L* ili a*/b* kanalima. L* treba da je [0,100](float), a a*b* [-1,1](float)\") \n",
    "        elif use_joined and np.all( (dataset_memory_map[i] == 0) | (dataset_memory_map[i] == 255) ):\n",
    "           warn(f\"Slika redni broj {i} sadrzi cudne vrednosti na RGB kanalima, opseg treba da je [0,255](uint8).\") \n",
    "\n",
    "    del dataset_memory_map_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "validation_size = 0.15\n",
    "\n",
    "test_length = int(test_size * dataset_memory_map.shape[0])\n",
    "remaining_length = dataset_memory_map.shape[0] - test_length\n",
    "validation_length = int(remaining_length * validation_size)\n",
    "training_length = remaining_length - validation_length\n",
    "\n",
    "training_length, validation_length, test_length\n",
    "\n",
    "indices = [i for i in range(dataset_memory_map.shape[0])]\n",
    "\n",
    "training_indices = np.random.choice(range(0, dataset_memory_map.shape[0]), size=training_length, replace=False)\n",
    "validation_indices = np.random.choice(list(set(range(0, dataset_memory_map.shape[0])) - set(training_indices)), size=validation_length, replace=False)\n",
    "test_indices = np.random.choice(list(set(range(0, dataset_memory_map.shape[0])) - set(training_indices) - set(validation_indices)), size=test_length, replace=False)\n",
    "\n",
    "# training_ds = IterableImageDataset(dataset_memory_map[training_indices], transform=transform)\n",
    "# validation_ds = IterableImageDataset(dataset_memory_map[validation_indices], transform=transform)\n",
    "training_ds = IterableImageDataset(training_indices,)\n",
    "validation_ds = IterableImageDataset(validation_indices,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_Joined = False\n",
    "# def process_data(dataset):\n",
    "#     for idx in training_indices:\n",
    "#         img = dataset[idx]\n",
    "#         if transform:\n",
    "#             img_transformed = transform(torch.as_tensor( convert_lab_to_rgb(img) if not use_joined else img,dtype=torch.float32))\n",
    "#             if use_joined:\n",
    "#                 img_transformed = convert_rgb_to_lab(img_transformed)\n",
    "#             yield img_transformed[0].reshape(-1,224,224), img_transformed[1:].reshape(-1,224,224)\n",
    "#         else:\n",
    "#             if use_joined:\n",
    "#                 img = convert_rgb_to_lab(img)\n",
    "#             yield img[0].reshape(-1,224,224), img[1:].reshape(-1,224,224)\n",
    "\n",
    "# for x,y in iter(process_data(dataset_memory_map)):\n",
    "#     print(x.shape)\n",
    "#     print(y.shape)\n",
    "#     break\n",
    "\n",
    "# for nesto in iter(map(process_data, iter(dataset_memory_map))):\n",
    "#     print(\"\\n\",nesto)\n",
    "#     break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizerModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ColorizerModel, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1, stride=2)\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=2)\n",
    "        self.conv7 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv8 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.conv9 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.upsample1 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.conv10 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.upsample2 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.conv11 = nn.Conv2d(64, 32, kernel_size=3, padding=1)\n",
    "        self.conv12 = nn.Conv2d(32, 2, kernel_size=3, padding=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.upsample3 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.last = nn.Conv2d(n_prev, n_classes, kernel_size=1, stride=1, padding=0)\n",
    "        self.model_output = nn.Conv2d(n_classes,2, kernel_size=1,padding=0,dilation=1,stride=1,bias=False)\n",
    "        self.upsalmple4 = nn.Upsample(scale_factor=4, mode='bilinear')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.ReLU()(self.conv1(x))\n",
    "        x = nn.ReLU()(self.conv2(x))\n",
    "        x = nn.ReLU()(self.conv3(x))\n",
    "        x = nn.ReLU()(self.conv4(x))\n",
    "        x = nn.ReLU()(self.conv5(x))\n",
    "        x = nn.ReLU()(self.conv6(x))\n",
    "        x = nn.ReLU()(self.conv7(x))\n",
    "        x = nn.ReLU()(self.conv8(x))\n",
    "        x = nn.ReLU()(self.conv9(x))\n",
    "        x = self.upsample1(x)\n",
    "        x = nn.ReLU()(self.conv10(x))\n",
    "        x = self.upsample2(x)\n",
    "        x = nn.ReLU()(self.conv11(x))\n",
    "        x = self.tanh(self.conv12(x))\n",
    "        x = self.upsample3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "def ReweightedCrossEntropy(torch.nn.Module):\n",
    "    def __init__(self, Q_value, class_weights, lambda_value=0.5, sigma_value=5):\n",
    "        super(ReweightedCrossEntropy, self).__init__()\n",
    "        self.lambda_value = lambda_value\n",
    "        self.sigma_value = sigma_value\n",
    "        self.Q_value = Q_value\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, model_predictions, ground_truth):\n",
    "\n",
    "        w_value = ( (1-self.lambda_value) * self.class_weights + (self.lambda_value/self.Q_value) ) ** -1\n",
    "\n",
    "        return cross_entropy(model_predictions,ground_truth,w_value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping(object):\n",
    "    def __init__(self, mode='min', min_delta=0, patience=10, percentage=False, also_use_timer=False, seconds_to_terminate:int=60*60):\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = 0\n",
    "        self.is_better = None\n",
    "        self.also_use_timer=also_use_timer\n",
    "        self._init_is_better(mode, min_delta, percentage)\n",
    "\n",
    "        if patience == 0:\n",
    "            self.is_better = lambda a, b: True\n",
    "            self.step = lambda a: False\n",
    "\n",
    "        if also_use_timer:\n",
    "            self.start_time=time.perf_counter()\n",
    "            self.end_time = 0\n",
    "            self.time_compare = lambda start,end: end-start >= seconds_to_terminate # NOTE Terminate after an hour\n",
    "        else:\n",
    "            self.start_time=None\n",
    "            self.end_time=None\n",
    "            self.time_compare = lambda start,end: False\n",
    "\n",
    "    def step(self, metrics):\n",
    "        if self.best is None:\n",
    "            self.best = metrics\n",
    "            return False\n",
    "\n",
    "        if torch.is_tensor(metrics):\n",
    "            if torch.isnan(metrics):\n",
    "                return True\n",
    "        elif type(metrics) == float and math.isnan(metrics):\n",
    "              return True\n",
    "\n",
    "        if self.is_better(metrics, self.best):\n",
    "            self.num_bad_epochs = 0\n",
    "            self.best = metrics\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            print('terminating because of early stopping!')\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def time_ran_out(self):\n",
    "        if self.time_compare(self.start_time, self.end_time):\n",
    "            print(\"Terminating because of training time limit.\")\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _init_is_better(self, mode, min_delta, percentage):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError('mode ' + mode + ' is unknown!')\n",
    "        if not percentage:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - min_delta\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + min_delta\n",
    "        else:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - (\n",
    "                            best * min_delta / 100)\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + (\n",
    "                            best * min_delta / 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for training and evaluating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, validation: DataLoader, device, metric, is_called_from_training=False):\n",
    "    model.eval()\n",
    "    fid = [] \n",
    "    for step, batch in enumerate(validation):\n",
    "        with torch.no_grad():\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            metric.update(torch.as_tensor(convert_lab_to_rgb(torch.cat( (inputs,targets*128), dim=1).cpu().numpy(),denormalize=True),dtype=torch.uint8), real=True)\n",
    "            metric.update(torch.as_tensor(convert_lab_to_rgb(torch.cat( (inputs,outputs*128), dim=1).cpu().numpy(),denormalize=True),dtype=torch.uint8), real=False)\n",
    "            fid.append(metric.compute().item())\n",
    "    model.train()\n",
    "    del batch\n",
    "    collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return fid\n",
    "\n",
    "def fit(model: nn.Module, optimizer: optim.Optimizer, training:DataLoader, validation: DataLoader, scheduler: lr_scheduler.LRScheduler, metric_for_early_stopping,  epochs:int=50, loss_fn=nn.MSELoss(), gradient_accumulation_steps:int=8,enable_early_stopping:bool=True,patience:int=7,early_stopping_mode:str='min',delta_for_early_stopping:float=0,best:float=None,also_use_timer_for_early_stopping:bool=False, seconds_for_early_stopping:int=60*60, device:str='cpu', epoch_start:int=0, loss=None, shouldEvaluate:bool = True):\n",
    "    \"\"\"\n",
    "    Trenira/fituje model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        Ovo je objekat instanciranog modela kojeg treniramo\n",
    "    optimizer : optim.Optimizer\n",
    "        Optimizator parametara `model` koje koristimo\n",
    "    training : DataLoader\n",
    "        DataLoader za trening\n",
    "    validation : DataLoader\n",
    "        DataLoader za validaciju\n",
    "    epochs : int, optional\n",
    "        Broj epoha prilikom treninga\n",
    "    loss_fn : optional\n",
    "        Funkcija za generisanja loss-a tokom treninga `model`-a.\n",
    "    scheduler : LRSCheduler \n",
    "        Scheduler za `learning_rate` \n",
    "    gradient_accumulation_steps : int, optional\n",
    "        Koliko step-ova akumuliramo gradijente pre nego sto uradimo apdejt vejtova. Ako ne zelimo akumuliranje gradijenata, setovati ovaj parametar na 1.\n",
    "    early_stopping_mode : str, optional\n",
    "        Rezim rada early stopping mehanizma(moze biti `min` ili `max`)\n",
    "    patience : int, optional\n",
    "        Koliko koraka u EarlyStoppingu tolerisemo pre nego sto prekinemo trening\n",
    "    delta_for_early_stopping : float, optional\n",
    "        Tolerancija odstupanja performansi za early stopping\n",
    "    metric_for_early_stopping : str, optional\n",
    "        Koju metriku cemo koristiti za early stopping. \n",
    "    best : float, optional\n",
    "        Najbolji rezultat koji je model postigao. Podrazumevano nema, ako instanciramo model od 0.\n",
    "    also_use_timer_for_early_stopping : bool, optional\n",
    "        Da li se koristi i tajmer za early stopping(ako npr. zelimo da trening traje odredjeno vreme)\n",
    "    device : {'cpu', 'cuda'}\n",
    "        Na kojem uredjaju zelimo da se vrsi trening.\n",
    "    epoch_start : int=0\n",
    "        Od koje epohe poceti trening, koristi se samo ako nastavljamo od checkpointa.\n",
    "    loss \n",
    "        Koji je loss bio na poslednjoj sacuvanoj epohi?\n",
    "    \"\"\"\n",
    "    # model = model.to(device)\n",
    "\n",
    "    trainingSteps = epochs * len(training)\n",
    "    \n",
    "    if enable_early_stopping:\n",
    "        earlyStopping = EarlyStopping(patience=min(epochs, patience), mode=early_stopping_mode, min_delta=delta_for_early_stopping,also_use_timer=also_use_timer_for_early_stopping, seconds_to_terminate=seconds_for_early_stopping)\n",
    "    best = best\n",
    "    \n",
    "    progress_bar = tqdm(range(trainingSteps))\n",
    "\n",
    "    model.train()\n",
    "    completed_steps = 0\n",
    "    # Ovo navodno ubrzava\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \"\"\"\n",
    "    Reason for turning off above:\n",
    "\n",
    "    Setting torch.backends.cudnn.benchmark = True before the training loop can accelerate the computation. Because the performance of cuDNN algorithms to compute the convolution of different kernel sizes varies, the auto-tuner can run a benchmark to find the best algorithm (current algorithms are [these](https://github.com/pytorch/pytorch/blob/dab5e2a23ed387046d99f825e0d9a45bd58fccaa/aten/src/ATen/native/cudnn/Conv_v7.cpp#L268-L275), [these](https://github.com/pytorch/pytorch/blob/dab5e2a23ed387046d99f825e0d9a45bd58fccaa/aten/src/ATen/native/cudnn/Conv_v7.cpp#L341-L346), and [these](https://github.com/pytorch/pytorch/blob/dab5e2a23ed387046d99f825e0d9a45bd58fccaa/aten/src/ATen/native/cudnn/Conv_v7.cpp#L413-L418)). It's recommended to use turn on the setting when your input size doesn't change often. If the input size changes often, the auto-tuner needs to benchmark too frequently, which might hurt the performance. It can speed up by 1.27x to 1.70x for forward and backward propagation [ref](https://tigress-web.princeton.edu/~jdh4/PyTorchPerformanceTuningGuide_GTC2021.pdf).    \n",
    "    \"\"\" \n",
    "    fid_over_training = dict()\n",
    "    losses = []\n",
    "    loss_now = loss\n",
    "    loss_before = None\n",
    "    for epoch in range(epoch_start, epochs):\n",
    "        for step, batch in enumerate(training, start=1):\n",
    "            # outputs = model(**batch)\n",
    "            # loss = outputs.loss\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            # print(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss_before = loss_now\n",
    "            loss_now = loss\n",
    "            losses.append(loss.item())\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            progress_bar.update(1)\n",
    "            if step % gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad(set_to_none=True) # set_to_none ne okida memset funkciju i navodno je to brze\n",
    "                completed_steps += 1\n",
    "        evaluation = evaluate(model, validation, device, metric_for_early_stopping, is_called_from_training=True) if shouldEvaluate else best\n",
    "        if shouldEvaluate:\n",
    "            evaluation = sum(evaluation) / len(evaluation)\n",
    "            fid_over_training[epoch] = evaluation\n",
    "        print(f\"Tokom epohe {epoch+1} loss je bio {loss} sa akumuliranjem, tj. {loss*gradient_accumulation_steps} bez akumuliranja gradijenta, learning rate je {scheduler.get_last_lr()}, a evaluacija je dala metriku {evaluation}\")\n",
    "        del batch\n",
    "        collect()\n",
    "        torch.cuda.empty_cache() \n",
    "        if enable_early_stopping:\n",
    "            earlyStopping.end_time = time.perf_counter()\n",
    "        if best is None or evaluation < best or not shouldEvaluate and loss_now < loss_before:\n",
    "            best = evaluation\n",
    "            torch.save({\n",
    "                \"model\":model.state_dict(),\n",
    "                f\"optimizer_{optimizer.__class__.__name__}\" : optimizer.state_dict(),\n",
    "                f\"scheduler_{scheduler.__class__.__name__}\" : scheduler.state_dict(),\n",
    "                \"epoch\": epoch,\n",
    "                \"loss\": loss.item() * gradient_accumulation_steps,\n",
    "                \"best\" : best\n",
    "            },model_weights_path + 'checkpoint.tar')\n",
    "            # torch.save(model.state_dict(), model_weights_path+'model_weights.pth') # NOTE: Mozda puca\n",
    "            # torch.save(optimizer.state_dict(), model_weights_path+f'optimizer_{optimizer.__class__.__name__}.pt') # NOTE: Mozda puca\n",
    "            # torch.save(scheduler.state_dict(), model_weights_path+f'scheduler_{scheduler.__class__.__name__}.pt') # NOTE: Mozda puca\n",
    "\n",
    "        if enable_early_stopping and (earlyStopping.step(evaluation) or earlyStopping.time_ran_out()):\n",
    "            break\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    return {\"fid\":fid_over_training, \"losses\":losses}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_epochs = 25\n",
    "batch_size = 32 \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ColorizerModel().to(device)\n",
    "optimizer = Lion(model.parameters(), lr=3.67*0.001)\n",
    "learning_rate_scheduler = lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=number_of_epochs * len(training_ds), eta_min=0.01)\n",
    "# optimizer = torch.optim.RMSprop(model.parameters())\n",
    "# learning_rate_scheduler = lr_scheduler.LinearLR(optimizer=optimizer, start_factor=0.9,end_factor=1/5,total_iters=number_of_epochs * len(training))\n",
    "\n",
    "#NOTE: menjaj ovo ako hoces da ucitas weightove\n",
    "use_pretrained_weights = True \n",
    "# training = DataLoader(training_ds, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "# validation = DataLoader(validation_ds, batch_size=int(batch_size/4), shuffle=True, pin_memory=True)\n",
    "training = DataLoader(training_ds, batch_size=batch_size, pin_memory=True)\n",
    "validation = DataLoader(validation_ds, batch_size=batch_size, pin_memory=True)\n",
    "metric = FrechetInceptionDistance(feature=64, reset_real_features=False)\n",
    "epoch_start=0\n",
    "loss_start=None\n",
    "best_result = None\n",
    "if use_pretrained_weights:\n",
    "    checkpoint = torch.load(model_weights_path+\"checkpoint.tar\", map_location=device)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint[f'optimizer_{optimizer.__class__.__name__}'])\n",
    "    for state in optimizer.state.values():\n",
    "        for key, value in state.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                state[key] = value.to(device)\n",
    "    learning_rate_scheduler.load_state_dict(checkpoint[f'scheduler_{learning_rate_scheduler.__class__.__name__}'])\n",
    "    # for state in learning_rate_scheduler.state_dict().values():\n",
    "    #     for key, value in state.items():\n",
    "    #         if isinstance(value, torch.Tensor):\n",
    "    #             state[key] = value.to(device)\n",
    "    epoch_start = checkpoint['epoch']\n",
    "    loss_start = checkpoint['loss'] \n",
    "    best_result = checkpoint['best']\n",
    "    model.eval()\n",
    "    # model.load_state_dict(torch.load(model_weights_path+ 'model_weights.pth'))\n",
    "    # optimizer.load_state_dict(torch.load(model_weights_path + f'optimizer_{optimizer.__class__.__name__}.pt', map_location=device))\n",
    "    # learning_rate_scheduler.load_state_dict(torch.load(model_weights_path + f'scheduler_{learning_rate_scheduler.__class__.__name__}.pt', map_location=device))\n",
    "\n",
    "del training_ds, validation_ds, dataset_memory_map\n",
    "collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 1.2506995797157288 None\n"
     ]
    }
   ],
   "source": [
    "print(epoch_start,loss_start,best_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric = FrechetInceptionDistance(feature=64, reset_real_features=False, normalize=True)\n",
    "# img_real_distribution = torch.rand( (64,3,224,224))\n",
    "# img_pred_distribution = torch.rand( (64,3,224,224))\n",
    "\n",
    "# metric.update(img_real_distribution,real=True)\n",
    "# metric.update(img_pred_distribution,real=False)\n",
    "# metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.to(device)\n",
    "# model.eval()\n",
    "# img_real_distribution = []\n",
    "# img_pred_distribution = []\n",
    "# for step, batch in enumerate(validation):\n",
    "#     with torch.no_grad():\n",
    "#         inputs, targets = batch\n",
    "#         if(not False):\n",
    "#             inputs, targets = inputs.to(device), targets.to(device)\n",
    "#         outputs = model(inputs)\n",
    "#         img_real_distribution.append(convert_lab_to_rgb(torch.cat( (inputs,targets*128), dim=1).cpu().numpy(),denormalize=True))#NOTE: mozda puca\n",
    "#         img_pred_distribution.append(convert_lab_to_rgb(torch.cat( (inputs,outputs*128), dim=1).cpu().numpy(),denormalize=True))#NOTE: mozda puca\n",
    "#     break\n",
    "# model.train()\n",
    "# metric.update(torch.ByteTensor(test_real), real=True)\n",
    "# metric.update(torch.ByteTensor(test_pred), real=False)\n",
    "# metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filterwarnings(\"ignore\")\n",
    "# model.to(device)\n",
    "# test_real, test_pred = evaluate(model,validation,device,metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Struktura i parametri modela prikazani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ColorizerModel(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv6): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv7): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv8): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv9): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (upsample1): Upsample(scale_factor=2.0, mode='nearest')\n",
      "  (conv10): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (upsample2): Upsample(scale_factor=2.0, mode='nearest')\n",
      "  (conv11): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv12): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (tanh): Tanh()\n",
      "  (upsample3): Upsample(scale_factor=2.0, mode='nearest')\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parametar:\tconv1.weight,\tTip:\ttorch.float32,\tVelicina:\t576\n",
      "Parametar:\tconv1.bias,\tTip:\ttorch.float32,\tVelicina:\t64\n",
      "Parametar:\tconv2.weight,\tTip:\ttorch.float32,\tVelicina:\t36864\n",
      "Parametar:\tconv2.bias,\tTip:\ttorch.float32,\tVelicina:\t64\n",
      "Parametar:\tconv3.weight,\tTip:\ttorch.float32,\tVelicina:\t73728\n",
      "Parametar:\tconv3.bias,\tTip:\ttorch.float32,\tVelicina:\t128\n",
      "Parametar:\tconv4.weight,\tTip:\ttorch.float32,\tVelicina:\t147456\n",
      "Parametar:\tconv4.bias,\tTip:\ttorch.float32,\tVelicina:\t128\n",
      "Parametar:\tconv5.weight,\tTip:\ttorch.float32,\tVelicina:\t294912\n",
      "Parametar:\tconv5.bias,\tTip:\ttorch.float32,\tVelicina:\t256\n",
      "Parametar:\tconv6.weight,\tTip:\ttorch.float32,\tVelicina:\t589824\n",
      "Parametar:\tconv6.bias,\tTip:\ttorch.float32,\tVelicina:\t256\n",
      "Parametar:\tconv7.weight,\tTip:\ttorch.float32,\tVelicina:\t1179648\n",
      "Parametar:\tconv7.bias,\tTip:\ttorch.float32,\tVelicina:\t512\n",
      "Parametar:\tconv8.weight,\tTip:\ttorch.float32,\tVelicina:\t1179648\n",
      "Parametar:\tconv8.bias,\tTip:\ttorch.float32,\tVelicina:\t256\n",
      "Parametar:\tconv9.weight,\tTip:\ttorch.float32,\tVelicina:\t294912\n",
      "Parametar:\tconv9.bias,\tTip:\ttorch.float32,\tVelicina:\t128\n",
      "Parametar:\tconv10.weight,\tTip:\ttorch.float32,\tVelicina:\t73728\n",
      "Parametar:\tconv10.bias,\tTip:\ttorch.float32,\tVelicina:\t64\n",
      "Parametar:\tconv11.weight,\tTip:\ttorch.float32,\tVelicina:\t18432\n",
      "Parametar:\tconv11.bias,\tTip:\ttorch.float32,\tVelicina:\t32\n",
      "Parametar:\tconv12.weight,\tTip:\ttorch.float32,\tVelicina:\t576\n",
      "Parametar:\tconv12.bias,\tTip:\ttorch.float32,\tVelicina:\t2\n",
      "--------------- \n",
      "Ukupno parametara:\t 3892194\n"
     ]
    }
   ],
   "source": [
    "total_params=0\n",
    "for name, parameter in model.named_parameters():\n",
    "    print(f\"Parametar:\\t{name},\\tTip:\\t{parameter.dtype},\\tVelicina:\\t{parameter.numel()}\")\n",
    "    total_params+=parameter.numel()\n",
    "print(\"-\"*15,\"\\nUkupno parametara:\\t\",total_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filterwarnings(\"ignore\")\n",
    "# model.to(device)\n",
    "# evaluate(model,validation,device,metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a540eac62ee476f99ad38d58441eaad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19550 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokom epohe 6 loss je bio 0.10509137809276581 sa akumuliranjem, tj. 1.261096477508545 bez akumuliranja gradijenta, learning rate je [0.0036700073586448875], a evaluacija je dala metriku None\n",
      "Tokom epohe 7 loss je bio 0.10422496497631073 sa akumuliranjem, tj. 1.250699520111084 bez akumuliranja gradijenta, learning rate je [0.0036700089455182537], a evaluacija je dala metriku None\n",
      "Tokom epohe 8 loss je bio 0.1033782958984375 sa akumuliranjem, tj. 1.24053955078125 bez akumuliranja gradijenta, learning rate je [0.003670010687208231], a evaluacija je dala metriku None\n",
      "Tokom epohe 9 loss je bio 0.10424895584583282 sa akumuliranjem, tj. 1.2509875297546387 bez akumuliranja gradijenta, learning rate je [0.003670012583714727], a evaluacija je dala metriku None\n",
      "Tokom epohe 10 loss je bio 0.10486376285552979 sa akumuliranjem, tj. 1.2583651542663574 bez akumuliranja gradijenta, learning rate je [0.0036700146350376545], a evaluacija je dala metriku None\n",
      "Tokom epohe 11 loss je bio 0.10442712157964706 sa akumuliranjem, tj. 1.2531254291534424 bez akumuliranja gradijenta, learning rate je [0.00367001684117691], a evaluacija je dala metriku None\n",
      "Tokom epohe 12 loss je bio 0.10311315953731537 sa akumuliranjem, tj. 1.2373578548431396 bez akumuliranja gradijenta, learning rate je [0.003670019202132383], a evaluacija je dala metriku None\n",
      "Tokom epohe 13 loss je bio 0.10373370349407196 sa akumuliranjem, tj. 1.2448043823242188 bez akumuliranja gradijenta, learning rate je [0.0036700217179039705], a evaluacija je dala metriku None\n",
      "Tokom epohe 14 loss je bio 0.10535068809986115 sa akumuliranjem, tj. 1.2642083168029785 bez akumuliranja gradijenta, learning rate je [0.0036700243884915384], a evaluacija je dala metriku None\n",
      "Tokom epohe 15 loss je bio 0.10465987026691437 sa akumuliranjem, tj. 1.2559185028076172 bez akumuliranja gradijenta, learning rate je [0.0036700272138949592], a evaluacija je dala metriku None\n",
      "Tokom epohe 16 loss je bio 0.10363657772541046 sa akumuliranjem, tj. 1.2436389923095703 bez akumuliranja gradijenta, learning rate je [0.003670030194114095], a evaluacija je dala metriku None\n",
      "Tokom epohe 17 loss je bio 0.10382869094610214 sa akumuliranjem, tj. 1.2459442615509033 bez akumuliranja gradijenta, learning rate je [0.0036700333291488016], a evaluacija je dala metriku None\n",
      "Tokom epohe 18 loss je bio 0.10484722256660461 sa akumuliranjem, tj. 1.2581666707992554 bez akumuliranja gradijenta, learning rate je [0.003670036618998922], a evaluacija je dala metriku None\n",
      "Tokom epohe 19 loss je bio 0.10325871407985687 sa akumuliranjem, tj. 1.2391045093536377 bez akumuliranja gradijenta, learning rate je [0.0036700400636642937], a evaluacija je dala metriku None\n",
      "Tokom epohe 20 loss je bio 0.10521933436393738 sa akumuliranjem, tj. 1.2626320123672485 bez akumuliranja gradijenta, learning rate je [0.003670043663144754], a evaluacija je dala metriku None\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m filterwarnings(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m fit(model\u001b[39m=\u001b[39;49mmodel,optimizer\u001b[39m=\u001b[39;49moptimizer,training\u001b[39m=\u001b[39;49mtraining, validation\u001b[39m=\u001b[39;49mvalidation,scheduler\u001b[39m=\u001b[39;49mlearning_rate_scheduler,epochs\u001b[39m=\u001b[39;49mnumber_of_epochs,device\u001b[39m=\u001b[39;49mdevice, gradient_accumulation_steps\u001b[39m=\u001b[39;49m\u001b[39m12\u001b[39;49m, metric_for_early_stopping\u001b[39m=\u001b[39;49mmetric, enable_early_stopping\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,early_stopping_mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmin\u001b[39;49m\u001b[39m'\u001b[39;49m,patience\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m,epoch_start\u001b[39m=\u001b[39;49mepoch_start, loss\u001b[39m=\u001b[39;49mloss_start, shouldEvaluate\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, best\u001b[39m=\u001b[39;49mbest_result)\n",
      "Cell \u001b[1;32mIn[22], line 84\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(model, optimizer, training, validation, scheduler, metric_for_early_stopping, epochs, loss_fn, gradient_accumulation_steps, enable_early_stopping, patience, early_stopping_mode, delta_for_early_stopping, best, also_use_timer_for_early_stopping, seconds_for_early_stopping, device, epoch_start, loss, shouldEvaluate)\u001b[0m\n\u001b[0;32m     82\u001b[0m loss_before \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epoch_start, epochs):\n\u001b[1;32m---> 84\u001b[0m     \u001b[39mfor\u001b[39;00m step, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(training, start\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m     85\u001b[0m         \u001b[39m# outputs = model(**batch)\u001b[39;00m\n\u001b[0;32m     86\u001b[0m         \u001b[39m# loss = outputs.loss\u001b[39;00m\n\u001b[0;32m     87\u001b[0m         inputs, targets \u001b[39m=\u001b[39m batch\n\u001b[0;32m     88\u001b[0m         inputs, targets \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device, non_blocking\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), targets\u001b[39m.\u001b[39mto(device, non_blocking\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Projects\\CompVisProject\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\Projects\\CompVisProject\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\Projects\\CompVisProject\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:32\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m possibly_batched_index:\n\u001b[0;32m     31\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 32\u001b[0m         data\u001b[39m.\u001b[39mappend(\u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset_iter))\n\u001b[0;32m     33\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mended \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 67\u001b[0m, in \u001b[0;36mIterableImageDataset.process_data\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m     65\u001b[0m img \u001b[39m=\u001b[39m dataset[idx]\n\u001b[0;32m     66\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform:\n\u001b[1;32m---> 67\u001b[0m     img_transformed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(torch\u001b[39m.\u001b[39mas_tensor( convert_lab_to_rgb(img) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_joined \u001b[39melse\u001b[39;00m img,dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32))\n\u001b[0;32m     68\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_joined:\n\u001b[0;32m     69\u001b[0m         img_transformed \u001b[39m=\u001b[39m convert_rgb_to_lab(img_transformed)\n",
      "Cell \u001b[1;32mIn[9], line 21\u001b[0m, in \u001b[0;36mconvert_lab_to_rgb\u001b[1;34m(image, denormalize)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[39massert\u001b[39;00m image\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPogresna dimenzija na poziciji shape[0], ocekivano 3, dobijeno \u001b[39m\u001b[39m{\u001b[39;00mimage\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     20\u001b[0m     \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39mall( (image[\u001b[39m0\u001b[39m] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m) \u001b[39m&\u001b[39m ( image[\u001b[39m0\u001b[39m] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m100.0\u001b[39m) \u001b[39m&\u001b[39m (image[\u001b[39m1\u001b[39m:] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m128\u001b[39m) \u001b[39m&\u001b[39m ( image[\u001b[39m1\u001b[39m:] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m128\u001b[39m) )\n\u001b[1;32m---> 21\u001b[0m     rgb_image \u001b[39m=\u001b[39m lab2rgb(image, channel_axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     22\u001b[0m \u001b[39melif\u001b[39;00m image\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[0;32m     23\u001b[0m     \u001b[39massert\u001b[39;00m image\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPogresna dimenzija na poziciji shape[1], ocekivano 3, dobijeno \u001b[39m\u001b[39m{\u001b[39;00mimage\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32md:\\Projects\\CompVisProject\\.venv\\lib\\site-packages\\skimage\\_shared\\utils.py:349\u001b[0m, in \u001b[0;36mchannel_as_last_axis.__call__.<locals>.fixed_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    346\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mchannel_axis\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m    348\u001b[0m \u001b[39m# Call the function with the fixed arguments\u001b[39;00m\n\u001b[1;32m--> 349\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39mnew_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    350\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmultichannel_output:\n\u001b[0;32m    351\u001b[0m     out \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmoveaxis(out, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, channel_axis[\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32md:\\Projects\\CompVisProject\\.venv\\lib\\site-packages\\skimage\\color\\colorconv.py:1301\u001b[0m, in \u001b[0;36mlab2rgb\u001b[1;34m(lab, illuminant, observer, channel_axis)\u001b[0m\n\u001b[0;32m   1295\u001b[0m \u001b[39mif\u001b[39;00m n_invalid \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1296\u001b[0m     warn(\n\u001b[0;32m   1297\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConversion from CIE-LAB, via XYZ to sRGB color space resulted in \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1298\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mn_invalid\u001b[39m}\u001b[39;00m\u001b[39m negative Z values that have been clipped to zero\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1299\u001b[0m         stacklevel\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,\n\u001b[0;32m   1300\u001b[0m     )\n\u001b[1;32m-> 1301\u001b[0m \u001b[39mreturn\u001b[39;00m xyz2rgb(xyz)\n",
      "File \u001b[1;32md:\\Projects\\CompVisProject\\.venv\\lib\\site-packages\\skimage\\_shared\\utils.py:316\u001b[0m, in \u001b[0;36mchannel_as_last_axis.__call__.<locals>.fixed_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    313\u001b[0m channel_axis \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mchannel_axis\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    315\u001b[0m \u001b[39mif\u001b[39;00m channel_axis \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 316\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    318\u001b[0m \u001b[39m# TODO: convert scalars to a tuple in anticipation of eventually\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \u001b[39m#       supporting a tuple of channel axes. Right now, only an\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \u001b[39m#       integer or a single-element tuple is supported, though.\u001b[39;00m\n\u001b[0;32m    321\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39misscalar(channel_axis):\n",
      "File \u001b[1;32md:\\Projects\\CompVisProject\\.venv\\lib\\site-packages\\skimage\\color\\colorconv.py:767\u001b[0m, in \u001b[0;36mxyz2rgb\u001b[1;34m(xyz, channel_axis)\u001b[0m\n\u001b[0;32m    765\u001b[0m arr \u001b[39m=\u001b[39m _convert(rgb_from_xyz, xyz)\n\u001b[0;32m    766\u001b[0m mask \u001b[39m=\u001b[39m arr \u001b[39m>\u001b[39m \u001b[39m0.0031308\u001b[39m\n\u001b[1;32m--> 767\u001b[0m arr[mask] \u001b[39m=\u001b[39m \u001b[39m1.055\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39;49mpower(arr[mask], \u001b[39m1\u001b[39;49m \u001b[39m/\u001b[39;49m \u001b[39m2.4\u001b[39;49m) \u001b[39m-\u001b[39m \u001b[39m0.055\u001b[39m\n\u001b[0;32m    768\u001b[0m arr[\u001b[39m~\u001b[39mmask] \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m \u001b[39m12.92\u001b[39m\n\u001b[0;32m    769\u001b[0m np\u001b[39m.\u001b[39mclip(arr, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, out\u001b[39m=\u001b[39marr)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filterwarnings(\"ignore\")\n",
    "fit(model=model,optimizer=optimizer,training=training, validation=validation,scheduler=learning_rate_scheduler,epochs=number_of_epochs,device=device, gradient_accumulation_steps=12, metric_for_early_stopping=metric, enable_early_stopping=True,early_stopping_mode='min',patience=4,epoch_start=epoch_start, loss=loss_start, shouldEvaluate=False, best=best_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO...\n",
    "# test = ImageDataset(dataset_memory_map[test_indices], transform=transform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorize(image: np.ndarray, print_image_inline=False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Ova metoda kolorizuje sliku koja joj je prosledjena.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.ndarray\n",
    "        Slika u RGB ili L*a*b* formatu(za sad).\n",
    "\n",
    "    print_image_inline : boolean=False\n",
    "        Da li da se slika prikaze inline ispod celije.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    \"\"\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upotreba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     output = model(torch.tensor(X).float().to(device)).cpu().numpy()\n",
    "#     output *= 128\n",
    "#     rgb_img = convert_lab_to_rgb(np.concatenate((X,output), axis=1),denormalize=True)\n",
    "#     imsave(\"img_result.png\", rgb_img[0].reshape(400,400,3))\n",
    "#     imsave(\"img_result_gray_version.png\", (255*rgb2gray(rgb_img[0].reshape(400,400,3))).astype(np.uint8))\n",
    "# model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
